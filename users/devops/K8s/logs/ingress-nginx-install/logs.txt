* 
* ==> Audit <==
* |------------|--------------------------------|----------|-------------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |       User        | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|-------------------|---------|---------------------|---------------------|
| start      |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 19 Dec 22 12:14 MSK | 19 Dec 22 12:25 MSK |
| start      | --driver virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 19 Dec 22 12:49 MSK |                     |
| delete     |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 19 Dec 22 12:51 MSK | 19 Dec 22 12:51 MSK |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 19 Dec 22 12:53 MSK |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 19 Dec 22 13:21 MSK | 19 Dec 22 13:28 MSK |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| ssh        |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 19 Dec 22 14:22 MSK | 19 Dec 22 14:24 MSK |
| kubectl    | get nodes                      | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 09:21 MSK |                     |
| kubectl    | get nodes                      | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 09:46 MSK |                     |
| dashboard  |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 09:53 MSK |                     |
| dashboard  |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 09:55 MSK |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 10:05 MSK | 20 Dec 22 10:09 MSK |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| dashboard  |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 10:10 MSK |                     |
| kubectl    | get nodes                      | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 10:21 MSK |                     |
| dashboard  | --url                          | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 10:25 MSK |                     |
| ssh        |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 10:35 MSK | 20 Dec 22 10:36 MSK |
| dashboard  |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 10:43 MSK |                     |
| docker-env | --shell powershell             | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 11:01 MSK |                     |
| ssh        |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 12:22 MSK |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 20 Dec 22 12:22 MSK |                     |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 21 Dec 22 08:56 MSK | 21 Dec 22 09:02 MSK |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| addons     | enable metrics-server          | minikube | IOURI-X555L\iurii | v1.28.0 | 21 Dec 22 09:17 MSK | 21 Dec 22 09:17 MSK |
| docker-env | --shell powershell             | minikube | IOURI-X555L\iurii | v1.28.0 | 21 Dec 22 13:12 MSK | 21 Dec 22 13:12 MSK |
| docker-env | minikube docker-env --shell    | minikube | IOURI-X555L\iurii | v1.28.0 | 21 Dec 22 13:15 MSK | 21 Dec 22 13:15 MSK |
|            | powershell                     |          |                   |         |                     |                     |
| docker-env | --shell powershell             | minikube | IOURI-X555L\iurii | v1.28.0 | 21 Dec 22 16:54 MSK | 21 Dec 22 16:54 MSK |
| docker-env | minikube docker-env --shell    | minikube | IOURI-X555L\iurii | v1.28.0 | 21 Dec 22 16:55 MSK | 21 Dec 22 16:55 MSK |
|            | powershell                     |          |                   |         |                     |                     |
| ssh        |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 21 Dec 22 16:58 MSK |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 08:23 MSK |                     |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 08:54 MSK |                     |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 09:00 MSK |                     |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 09:35 MSK |                     |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 09:43 MSK | 22 Dec 22 09:48 MSK |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| docker-env | --shell powershell             | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 09:59 MSK | 22 Dec 22 09:59 MSK |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 13:00 MSK | 22 Dec 22 13:10 MSK |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| docker-env | --shell powershell             | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 13:48 MSK | 22 Dec 22 13:48 MSK |
| docker-env | minikube docker-env --shell    | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 13:49 MSK | 22 Dec 22 13:50 MSK |
|            | powershell                     |          |                   |         |                     |                     |
| docker-env | minikube docker-env --shell    | minikube | IOURI-X555L\iurii | v1.28.0 | 22 Dec 22 13:51 MSK | 22 Dec 22 13:51 MSK |
|            | powershell                     |          |                   |         |                     |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 23 Dec 22 09:55 MSK | 23 Dec 22 09:59 MSK |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| service    | users-pg-db-service            | minikube | IOURI-X555L\iurii | v1.28.0 | 23 Dec 22 14:45 MSK | 23 Dec 22 14:45 MSK |
| service    | users-app-service              | minikube | IOURI-X555L\iurii | v1.28.0 | 23 Dec 22 17:04 MSK | 23 Dec 22 17:04 MSK |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 07:35 MSK |                     |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 08:01 MSK | 24 Dec 22 08:07 MSK |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| service    | users-app-service              | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 08:10 MSK | 24 Dec 22 08:10 MSK |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 09:02 MSK | 24 Dec 22 09:07 MSK |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| addons     | enable metrics-server          | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 09:13 MSK | 24 Dec 22 09:13 MSK |
| dashboard  |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 09:14 MSK |                     |
| service    | users-app-service              | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 09:23 MSK | 24 Dec 22 09:23 MSK |
| stop       |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 09:55 MSK | 24 Dec 22 09:56 MSK |
| ip         |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 09:56 MSK |                     |
| start      | --driver=virtualbox            | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 10:03 MSK | 24 Dec 22 10:08 MSK |
|            | --no-vtx-check                 |          |                   |         |                     |                     |
| service    | users-pg-db-service            | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 10:13 MSK | 24 Dec 22 10:13 MSK |
| service    | users-app-service              | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 10:21 MSK | 24 Dec 22 10:21 MSK |
| service    | users-app-service              | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 12:49 MSK | 24 Dec 22 12:49 MSK |
| dashboard  |                                | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 12:50 MSK |                     |
| addons     | enable ingress                 | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 13:19 MSK |                     |
| addons     | enable ingress                 | minikube | IOURI-X555L\iurii | v1.28.0 | 24 Dec 22 13:57 MSK |                     |
|------------|--------------------------------|----------|-------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/12/24 10:03:19
Running on machine: IOURI-X555L
Binary: Built with gc go1.19.2 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1224 10:03:19.259854   15264 out.go:296] Setting OutFile to fd 12 ...
I1224 10:03:19.368203   15264 out.go:343] TERM=,COLORTERM=, which probably does not support color
I1224 10:03:19.368203   15264 out.go:309] Setting ErrFile to fd 16...
I1224 10:03:19.368203   15264 out.go:343] TERM=,COLORTERM=, which probably does not support color
I1224 10:03:19.414980   15264 out.go:303] Setting JSON to false
I1224 10:03:19.428475   15264 start.go:116] hostinfo: {"hostname":"IOURI-X555L","uptime":5279,"bootTime":1671860120,"procs":270,"os":"windows","platform":"Microsoft Windows 10 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.19044 Build 19044","kernelVersion":"10.0.19044 Build 19044","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"5368af9a-d24c-4429-95cf-75502416b7fb"}
W1224 10:03:19.428475   15264 start.go:124] gopshost.Virtualization returned error: not implemented yet
I1224 10:03:19.466499   15264 out.go:177] * minikube v1.28.0 на Microsoft Windows 10 Home Single Language 10.0.19044 Build 19044
I1224 10:03:19.499745   15264 notify.go:220] Checking for updates...
I1224 10:03:19.500654   15264 config.go:180] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1224 10:03:19.514892   15264 driver.go:365] Setting default libvirt URI to qemu:///system
I1224 10:03:19.610008   15264 virtualbox.go:136] virtual box version: 7.0.4r154605
I1224 10:03:19.645019   15264 out.go:177] * Используется драйвер virtualbox на основе существующего профиля
I1224 10:03:19.720607   15264 start.go:282] selected driver: virtualbox
I1224 10:03:19.720607   15264 start.go:808] validating driver "virtualbox" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.28.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:true DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.59.102 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\iurii:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1224 10:03:19.720607   15264 start.go:819] status for virtualbox: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:7.0.4r154605
}
I1224 10:03:19.963131   15264 cni.go:95] Creating CNI manager for ""
I1224 10:03:19.963131   15264 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1224 10:03:19.963131   15264 start_flags.go:317] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.28.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:true DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.59.102 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\iurii:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1224 10:03:19.963696   15264 iso.go:124] acquiring lock: {Name:mk8e642c90d75ac537b42ed85eac6fd10742b6cf Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1224 10:03:20.030757   15264 out.go:177] * Запускается control plane узел minikube в кластере minikube
I1224 10:03:20.057526   15264 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1224 10:03:20.057526   15264 preload.go:148] Found local preload: C:\Users\iurii\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4
I1224 10:03:20.058061   15264 cache.go:57] Caching tarball of preloaded images
I1224 10:03:20.058646   15264 preload.go:174] Found C:\Users\iurii\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.25.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1224 10:03:20.058646   15264 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.3 on docker
I1224 10:03:20.059242   15264 profile.go:148] Saving config to C:\Users\iurii\.minikube\profiles\minikube\config.json ...
I1224 10:03:20.063852   15264 cache.go:208] Successfully downloaded all kic artifacts
I1224 10:03:20.064415   15264 start.go:364] acquiring machines lock for minikube: {Name:mk02bf21a9bfec333ade3c8e466e15f0727170ad Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I1224 10:03:20.064973   15264 start.go:368] acquired machines lock for "minikube" in 24.8µs
I1224 10:03:20.064973   15264 start.go:96] Skipping create...Using existing machine configuration
I1224 10:03:20.064973   15264 fix.go:55] fixHost starting: 
I1224 10:03:20.066100   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:03:20.217851   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="poweroff"
VMStateChangeTime="2022-12-24T06:56:02.524000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
}
I1224 10:03:20.217851   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:20.217851   15264 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1224 10:03:20.217851   15264 fix.go:129] unexpected machine state, will restart: <nil>
I1224 10:03:20.263857   15264 out.go:177] * Перезагружается существующий virtualbox VM для "minikube" ...
I1224 10:03:20.290846   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:03:20.435259   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="poweroff"
VMStateChangeTime="2022-12-24T06:56:02.524000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
}
I1224 10:03:20.435811   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:20.435811   15264 main.go:134] libmachine: Check network to re-create if needed...
I1224 10:03:20.435811   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe list hostonlyifs
I1224 10:03:20.976381   15264 main.go:134] libmachine: STDOUT:
{
Name:            VirtualBox Host-Only Ethernet Adapter
GUID:            351e50a6-a7f2-4b6d-9920-fd0d39de2864
DHCP:            Disabled
IPAddress:       192.168.56.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::e1f1:4a89:aee7:843f
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:09
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter

Name:            VirtualBox Host-Only Ethernet Adapter #2
GUID:            3ce34164-a7f4-48bb-a704-d05c4dd497e1
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::5157:bc45:f730:fcf8
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:0b
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter #2

}
I1224 10:03:20.976381   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:21.243040   15264 main.go:134] libmachine: Searching for hostonly interface for IPv4: 192.168.59.1 and Mask: ffffff00
I1224 10:03:21.243040   15264 main.go:134] libmachine: Found: VirtualBox Host-Only Ethernet Adapter #2
I1224 10:03:21.254900   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe list dhcpservers
I1224 10:03:21.347554   15264 main.go:134] libmachine: STDOUT:
{
NetworkName:    HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter
Dhcpd IP:       192.168.56.100
LowerIPAddress: 192.168.56.101
UpperIPAddress: 192.168.56.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None

NetworkName:    HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter #2
Dhcpd IP:       192.168.59.18
LowerIPAddress: 192.168.59.100
UpperIPAddress: 192.168.59.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None
}
I1224 10:03:21.347554   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:21.347554   15264 main.go:134] libmachine: Removing orphan DHCP servers...
I1224 10:03:21.347554   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe list hostonlyifs
I1224 10:03:21.539385   15264 main.go:134] libmachine: STDOUT:
{
Name:            VirtualBox Host-Only Ethernet Adapter
GUID:            351e50a6-a7f2-4b6d-9920-fd0d39de2864
DHCP:            Disabled
IPAddress:       192.168.56.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::e1f1:4a89:aee7:843f
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:09
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter

Name:            VirtualBox Host-Only Ethernet Adapter #2
GUID:            3ce34164-a7f4-48bb-a704-d05c4dd497e1
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::5157:bc45:f730:fcf8
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:0b
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter #2

}
I1224 10:03:21.539749   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:21.539749   15264 main.go:134] libmachine: Adding/Modifying DHCP server "192.168.59.2" with address range "192.168.59.100" - "192.168.59.254"...
I1224 10:03:21.539749   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe list dhcpservers
I1224 10:03:21.619141   15264 main.go:134] libmachine: STDOUT:
{
NetworkName:    HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter
Dhcpd IP:       192.168.56.100
LowerIPAddress: 192.168.56.101
UpperIPAddress: 192.168.56.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None

NetworkName:    HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter #2
Dhcpd IP:       192.168.59.18
LowerIPAddress: 192.168.59.100
UpperIPAddress: 192.168.59.254
NetworkMask:    255.255.255.0
Enabled:        Yes
Global Configuration:
    minLeaseTime:     default
    defaultLeaseTime: default
    maxLeaseTime:     default
    Forced options:   None
    Suppressed opts.: None
        1/legacy: 255.255.255.0
Groups:               None
Individual Configs:   None
}
I1224 10:03:21.619141   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:21.619141   15264 main.go:134] libmachine: Windows might ask for the permission to configure a dhcp server. Sometimes, such confirmation window is minimized in the taskbar.
I1224 10:03:21.619141   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe dhcpserver modify --netname HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter #2 --ip 192.168.59.2 --netmask 255.255.255.0 --lowerip 192.168.59.100 --upperip 192.168.59.254 --enable
I1224 10:03:21.841255   15264 main.go:134] libmachine: STDOUT:
{
}
I1224 10:03:21.841255   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:21.841255   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe modifyvm minikube --nic2 hostonly --nictype2 virtio --nicpromisc2 deny --hostonlyadapter2 VirtualBox Host-Only Ethernet Adapter #2 --cableconnected2 on
I1224 10:03:22.082002   15264 main.go:134] libmachine: STDOUT:
{
}
I1224 10:03:22.082002   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:22.082002   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe modifyvm minikube --natpf1 delete ssh
I1224 10:03:22.286521   15264 main.go:134] libmachine: STDOUT:
{
}
I1224 10:03:22.286521   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:22.286521   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe modifyvm minikube --natpf1 ssh,tcp,127.0.0.1,57780,,22
I1224 10:03:22.401301   15264 main.go:134] libmachine: STDOUT:
{
}
I1224 10:03:22.401301   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:22.401301   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe startvm minikube --type headless
I1224 10:03:33.411087   15264 main.go:134] libmachine: STDOUT:
{
Waiting for VM "minikube" to power on...
VM "minikube" has been successfully started.
}
I1224 10:03:33.411087   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:03:33.411087   15264 main.go:134] libmachine: Waiting for an IP...
I1224 10:03:33.411087   15264 main.go:134] libmachine: Getting to WaitForSSH function...
I1224 10:03:33.467548   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:03:33.481085   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:03:33.481085   15264 main.go:134] libmachine: About to run SSH command:
exit 0
I1224 10:04:49.468764   15264 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:64801->127.0.0.1:57780: wsarecv: An existing connection was forcibly closed by the remote host.
I1224 10:04:52.747766   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1224 10:04:52.747766   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:04:52.878959   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:04:52.878959   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:04:52.878959   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:04:52.996115   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:04:52.996115   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:04:52.996683   15264 main.go:134] libmachine: Host-only MAC: 08002772f78f

I1224 10:04:53.026516   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:04:53.027456   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:04:53.027456   15264 main.go:134] libmachine: About to run SSH command:
ip addr show
I1224 10:04:53.246731   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I1224 10:04:53.247708   15264 main.go:134] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86389sec preferred_lft 86389sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 589sec preferred_lft 589sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I1224 10:04:53.247708   15264 main.go:134] libmachine: IP is 192.168.59.102
I1224 10:04:53.247708   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:04:53.375611   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:04:53.375642   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:04:53.375698   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:04:53.496710   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:04:53.496710   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:04:53.497903   15264 main.go:134] libmachine: Host-only MAC: 08002772f78f

I1224 10:04:53.543660   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:04:53.544255   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:04:53.544367   15264 main.go:134] libmachine: About to run SSH command:
ip addr show
I1224 10:04:53.775788   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86388sec preferred_lft 86388sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 588sec preferred_lft 588sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I1224 10:04:53.775788   15264 main.go:134] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86388sec preferred_lft 86388sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 588sec preferred_lft 588sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I1224 10:04:53.776319   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe list hostonlyifs
I1224 10:04:54.055375   15264 main.go:134] libmachine: STDOUT:
{
Name:            VirtualBox Host-Only Ethernet Adapter
GUID:            351e50a6-a7f2-4b6d-9920-fd0d39de2864
DHCP:            Disabled
IPAddress:       192.168.56.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::e1f1:4a89:aee7:843f
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:09
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter

Name:            VirtualBox Host-Only Ethernet Adapter #2
GUID:            3ce34164-a7f4-48bb-a704-d05c4dd497e1
DHCP:            Disabled
IPAddress:       192.168.59.1
NetworkMask:     255.255.255.0
IPV6Address:     fe80::5157:bc45:f730:fcf8
IPV6NetworkMaskPrefixLength: 64
HardwareAddress: 0a:00:27:00:00:0b
MediumType:      Ethernet
Wireless:        No
Status:          Up
VBoxNetworkName: HostInterfaceNetworking-VirtualBox Host-Only Ethernet Adapter #2

}
I1224 10:04:54.055375   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:04:54.276966   15264 main.go:134] libmachine: Found: VirtualBox Host-Only Ethernet Adapter #2
I1224 10:04:54.315863   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:04:54.503866   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:04:54.503866   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:04:54.504387   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:04:54.762726   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:04:54.762726   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:04:54.763700   15264 main.go:134] libmachine: Host-only MAC: 08002772f78f

I1224 10:04:54.816227   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:04:54.817208   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:04:54.817208   15264 main.go:134] libmachine: About to run SSH command:
ip addr show
I1224 10:04:55.058191   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86387sec preferred_lft 86387sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 587sec preferred_lft 587sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I1224 10:04:55.058224   15264 main.go:134] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86387sec preferred_lft 86387sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 587sec preferred_lft 587sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I1224 10:04:55.058768   15264 profile.go:148] Saving config to C:\Users\iurii\.minikube\profiles\minikube\config.json ...
I1224 10:04:55.064310   15264 machine.go:88] provisioning docker machine ...
I1224 10:04:55.088024   15264 buildroot.go:166] provisioning hostname "minikube"
I1224 10:04:55.116221   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:04:55.116221   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:04:55.116221   15264 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1224 10:04:55.393844   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1224 10:04:55.432433   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:04:55.433077   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:04:55.433077   15264 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1224 10:04:55.703154   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1224 10:04:55.707980   15264 buildroot.go:172] set auth options {CertDir:C:\Users\iurii\.minikube CaCertPath:C:\Users\iurii\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\iurii\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\iurii\.minikube\machines\server.pem ServerKeyPath:C:\Users\iurii\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\iurii\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\iurii\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\iurii\.minikube}
I1224 10:04:55.707980   15264 buildroot.go:174] setting up certificates
I1224 10:04:55.707980   15264 provision.go:83] configureAuth start
I1224 10:04:55.707980   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:04:55.843957   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:04:55.843957   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:04:55.843957   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:04:56.031176   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:04:56.031176   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:04:56.031928   15264 main.go:134] libmachine: Host-only MAC: 08002772f78f

I1224 10:04:56.084817   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:04:56.084883   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:04:56.085398   15264 main.go:134] libmachine: About to run SSH command:
ip addr show
I1224 10:04:56.412760   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86386sec preferred_lft 86386sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 586sec preferred_lft 586sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

I1224 10:04:56.412760   15264 main.go:134] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86386sec preferred_lft 86386sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 586sec preferred_lft 586sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0

END SSH

I1224 10:04:56.454412   15264 provision.go:138] copyHostCerts
I1224 10:04:56.466774   15264 exec_runner.go:144] found C:\Users\iurii\.minikube/ca.pem, removing ...
I1224 10:04:56.466774   15264 exec_runner.go:207] rm: C:\Users\iurii\.minikube\ca.pem
I1224 10:04:56.474270   15264 exec_runner.go:151] cp: C:\Users\iurii\.minikube\certs\ca.pem --> C:\Users\iurii\.minikube/ca.pem (1074 bytes)
I1224 10:04:56.492267   15264 exec_runner.go:144] found C:\Users\iurii\.minikube/cert.pem, removing ...
I1224 10:04:56.492267   15264 exec_runner.go:207] rm: C:\Users\iurii\.minikube\cert.pem
I1224 10:04:56.493400   15264 exec_runner.go:151] cp: C:\Users\iurii\.minikube\certs\cert.pem --> C:\Users\iurii\.minikube/cert.pem (1119 bytes)
I1224 10:04:56.496502   15264 exec_runner.go:144] found C:\Users\iurii\.minikube/key.pem, removing ...
I1224 10:04:56.496502   15264 exec_runner.go:207] rm: C:\Users\iurii\.minikube\key.pem
I1224 10:04:56.497463   15264 exec_runner.go:151] cp: C:\Users\iurii\.minikube\certs\key.pem --> C:\Users\iurii\.minikube/key.pem (1679 bytes)
I1224 10:04:56.500417   15264 provision.go:112] generating server cert: C:\Users\iurii\.minikube\machines\server.pem ca-key=C:\Users\iurii\.minikube\certs\ca.pem private-key=C:\Users\iurii\.minikube\certs\ca-key.pem org=iurii.minikube san=[192.168.59.102 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1224 10:04:56.782297   15264 provision.go:172] copyRemoteCerts
I1224 10:04:56.853857   15264 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1224 10:04:56.853857   15264 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57780 SSHKeyPath:C:\Users\iurii\.minikube\machines\minikube\id_rsa Username:docker}
I1224 10:04:57.016105   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1224 10:04:57.135438   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I1224 10:04:57.247926   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1224 10:04:57.367564   15264 provision.go:86] duration metric: configureAuth took 1.6595833s
I1224 10:04:57.367564   15264 buildroot.go:189] setting minikube options for container-runtime
I1224 10:04:57.368773   15264 config.go:180] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1224 10:04:57.406688   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:04:57.407251   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:04:57.407251   15264 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1224 10:04:57.720061   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: tmpfs

I1224 10:04:57.720061   15264 buildroot.go:70] root file system type: tmpfs
I1224 10:04:57.742077   15264 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1224 10:04:57.775284   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:04:57.775284   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:04:57.776257   15264 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=virtualbox --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1224 10:04:58.065584   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=virtualbox --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1224 10:04:58.155508   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:04:58.156482   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:04:58.156482   15264 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1224 10:05:06.228732   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.

I1224 10:05:06.228732   15264 machine.go:91] provisioned docker machine in 11.1644222s
I1224 10:05:06.228804   15264 start.go:300] post-start starting for "minikube" (driver="virtualbox")
I1224 10:05:06.228804   15264 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1224 10:05:06.320877   15264 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1224 10:05:06.321850   15264 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57780 SSHKeyPath:C:\Users\iurii\.minikube\machines\minikube\id_rsa Username:docker}
I1224 10:05:06.685354   15264 ssh_runner.go:195] Run: cat /etc/os-release
I1224 10:05:06.716328   15264 info.go:137] Remote host: Buildroot 2021.02.12
I1224 10:05:06.768509   15264 filesync.go:126] Scanning C:\Users\iurii\.minikube\addons for local assets ...
I1224 10:05:06.769474   15264 filesync.go:126] Scanning C:\Users\iurii\.minikube\files for local assets ...
I1224 10:05:06.770455   15264 start.go:303] post-start completed in 541.6507ms
I1224 10:05:06.770455   15264 fix.go:57] fixHost completed within 1m46.7054816s
I1224 10:05:06.799747   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:05:06.800724   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:05:06.800724   15264 main.go:134] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I1224 10:05:07.042716   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: 1671865507.033318544

I1224 10:05:07.042716   15264 fix.go:207] guest clock: 1671865507.033318544
I1224 10:05:07.042716   15264 fix.go:220] Guest: 2022-12-24 10:05:07.033318544 +0300 MSK Remote: 2022-12-24 10:05:06.7704555 +0300 MSK m=+107.938358601 (delta=262.863044ms)
I1224 10:05:07.060844   15264 fix.go:191] guest clock delta is within tolerance: 262.863044ms
I1224 10:05:07.060844   15264 start.go:83] releasing machines lock for "minikube", held for 1m46.9958701s
I1224 10:05:07.061387   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:05:07.193199   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:05:07.193199   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:05:07.193199   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:05:07.318537   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:05:07.318537   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:05:07.319107   15264 main.go:134] libmachine: Host-only MAC: 08002772f78f

I1224 10:05:07.347676   15264 main.go:134] libmachine: Using SSH client type: native
I1224 10:05:07.347676   15264 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x121bd60] 0x121ece0 <nil>  [] 0s} 127.0.0.1 57780 <nil> <nil>}
I1224 10:05:07.347676   15264 main.go:134] libmachine: About to run SSH command:
ip addr show
I1224 10:05:07.584791   15264 main.go:134] libmachine: SSH cmd err, output: <nil>: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86374sec preferred_lft 86374sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 574sec preferred_lft 574sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:5f:47:bd:55 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

I1224 10:05:07.584791   15264 main.go:134] libmachine: SSH returned: 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:03:ea:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic eth0
       valid_lft 86374sec preferred_lft 86374sec
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:72:f7:8f brd ff:ff:ff:ff:ff:ff
    inet 192.168.59.102/24 brd 192.168.59.255 scope global dynamic eth1
       valid_lft 574sec preferred_lft 574sec
4: sit0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000
    link/sit 0.0.0.0 brd 0.0.0.0
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:5f:47:bd:55 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

END SSH

I1224 10:05:07.640428   15264 out.go:177] * Found network options:
I1224 10:05:07.658758   15264 out.go:177]   - NO_PROXY=localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24
W1224 10:05:07.685119   15264 proxy.go:119] fail to check proxy env: Error ip not in block
W1224 10:05:07.686097   15264 proxy.go:119] fail to check proxy env: Error ip not in block
W1224 10:05:07.686097   15264 proxy.go:119] fail to check proxy env: Error ip not in block
I1224 10:05:07.702700   15264 out.go:177]   - no_proxy=localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24
W1224 10:05:07.730012   15264 proxy.go:119] fail to check proxy env: Error ip not in block
W1224 10:05:07.730012   15264 proxy.go:119] fail to check proxy env: Error ip not in block
W1224 10:05:07.730012   15264 proxy.go:119] fail to check proxy env: Error ip not in block
W1224 10:05:07.732129   15264 proxy.go:119] fail to check proxy env: Error ip not in block
W1224 10:05:07.732129   15264 proxy.go:119] fail to check proxy env: Error ip not in block
W1224 10:05:07.732129   15264 proxy.go:119] fail to check proxy env: Error ip not in block
I1224 10:05:07.767063   15264 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1224 10:05:07.767596   15264 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57780 SSHKeyPath:C:\Users\iurii\.minikube\machines\minikube\id_rsa Username:docker}
I1224 10:05:07.821532   15264 ssh_runner.go:195] Run: systemctl --version
I1224 10:05:07.821532   15264 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57780 SSHKeyPath:C:\Users\iurii\.minikube\machines\minikube\id_rsa Username:docker}
I1224 10:05:10.297602   15264 ssh_runner.go:235] Completed: systemctl --version: (2.4760702s)
I1224 10:05:10.297602   15264 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.5305389s)
W1224 10:05:10.297602   15264 start.go:747] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
W1224 10:05:10.298188   15264 out.go:239] ! This VM is having trouble accessing https://registry.k8s.io
W1224 10:05:10.299159   15264 out.go:239] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1224 10:05:10.322139   15264 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1224 10:05:10.403697   15264 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1224 10:05:10.644196   15264 docker.go:613] Got preloaded images: -- stdout --
yuryli/k8s-users-app:11
yuryli/k8s-users-pg-db:14.6
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
kubernetesui/dashboard:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/metrics-server/metrics-server:<none>
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1224 10:05:10.644196   15264 docker.go:543] Images already preloaded, skipping extraction
I1224 10:05:10.694973   15264 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1224 10:05:10.886974   15264 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1224 10:05:11.092501   15264 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1224 10:05:11.242650   15264 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I1224 10:05:11.651323   15264 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1224 10:05:11.758713   15264 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1224 10:05:11.899866   15264 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1224 10:05:12.446064   15264 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1224 10:05:13.100498   15264 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1224 10:05:13.623560   15264 ssh_runner.go:195] Run: sudo systemctl restart docker
I1224 10:05:18.212708   15264 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.5891478s)
I1224 10:05:18.297920   15264 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1224 10:05:19.004284   15264 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1224 10:05:19.881842   15264 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1224 10:05:20.031244   15264 start.go:451] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1224 10:05:20.131211   15264 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1224 10:05:20.159328   15264 start.go:472] Will wait 60s for crictl version
I1224 10:05:20.216489   15264 ssh_runner.go:195] Run: sudo crictl version
I1224 10:05:21.261568   15264 ssh_runner.go:235] Completed: sudo crictl version: (1.0450793s)
I1224 10:05:21.286933   15264 start.go:481] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.20
RuntimeApiVersion:  1.41.0
I1224 10:05:21.345314   15264 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1224 10:05:21.718996   15264 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1224 10:05:22.003865   15264 out.go:204] * Подготавливается Kubernetes v1.25.3 на Docker 20.10.20 ...
I1224 10:05:22.030161   15264 out.go:177]   - env NO_PROXY=localhost,127.0.0.1,10.96.0.0/12,192.168.59.0/24,192.168.49.0/24,192.168.39.0/24
I1224 10:05:22.435510   15264 ssh_runner.go:195] Run: grep 192.168.59.1	host.minikube.internal$ /etc/hosts
I1224 10:05:22.460355   15264 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.59.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1224 10:05:22.542641   15264 preload.go:132] Checking if preload exists for k8s version v1.25.3 and runtime docker
I1224 10:05:22.594316   15264 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1224 10:05:22.733733   15264 docker.go:613] Got preloaded images: -- stdout --
yuryli/k8s-users-app:11
yuryli/k8s-users-pg-db:14.6
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
kubernetesui/dashboard:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/metrics-server/metrics-server:<none>
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1224 10:05:22.733733   15264 docker.go:543] Images already preloaded, skipping extraction
I1224 10:05:22.772060   15264 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1224 10:05:22.912816   15264 docker.go:613] Got preloaded images: -- stdout --
yuryli/k8s-users-app:11
yuryli/k8s-users-pg-db:14.6
registry.k8s.io/kube-apiserver:v1.25.3
registry.k8s.io/kube-scheduler:v1.25.3
registry.k8s.io/kube-controller-manager:v1.25.3
registry.k8s.io/kube-proxy:v1.25.3
kubernetesui/dashboard:<none>
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/metrics-server/metrics-server:<none>
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1224 10:05:22.912816   15264 cache_images.go:84] Images are preloaded, skipping loading
I1224 10:05:22.950496   15264 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1224 10:05:23.159561   15264 cni.go:95] Creating CNI manager for ""
I1224 10:05:23.159561   15264 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1224 10:05:23.172249   15264 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1224 10:05:23.172249   15264 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.59.102 APIServerPort:8443 KubernetesVersion:v1.25.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.59.102"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.59.102 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I1224 10:05:23.172249   15264 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.59.102
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.59.102
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.59.102"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1224 10:05:23.197215   15264 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.59.102 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1224 10:05:23.247005   15264 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.3
I1224 10:05:23.317118   15264 binaries.go:44] Found k8s binaries, skipping transfer
I1224 10:05:23.375327   15264 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1224 10:05:23.424727   15264 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (472 bytes)
I1224 10:05:23.528243   15264 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1224 10:05:23.649593   15264 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2036 bytes)
I1224 10:05:23.814650   15264 ssh_runner.go:195] Run: grep 192.168.59.102	control-plane.minikube.internal$ /etc/hosts
I1224 10:05:23.831628   15264 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.59.102	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1224 10:05:23.904817   15264 certs.go:54] Setting up C:\Users\iurii\.minikube\profiles\minikube for IP: 192.168.59.102
I1224 10:05:23.931273   15264 certs.go:182] skipping minikubeCA CA generation: C:\Users\iurii\.minikube\ca.key
I1224 10:05:23.961322   15264 certs.go:182] skipping proxyClientCA CA generation: C:\Users\iurii\.minikube\proxy-client-ca.key
I1224 10:05:24.014949   15264 certs.go:298] skipping minikube-user signed cert generation: C:\Users\iurii\.minikube\profiles\minikube\client.key
I1224 10:05:24.026198   15264 certs.go:298] skipping minikube signed cert generation: C:\Users\iurii\.minikube\profiles\minikube\apiserver.key.b76a7398
I1224 10:05:24.035408   15264 certs.go:298] skipping aggregator signed cert generation: C:\Users\iurii\.minikube\profiles\minikube\proxy-client.key
I1224 10:05:24.037357   15264 certs.go:388] found cert: C:\Users\iurii\.minikube\certs\C:\Users\iurii\.minikube\certs\ca-key.pem (1675 bytes)
I1224 10:05:24.038333   15264 certs.go:388] found cert: C:\Users\iurii\.minikube\certs\C:\Users\iurii\.minikube\certs\ca.pem (1074 bytes)
I1224 10:05:24.038333   15264 certs.go:388] found cert: C:\Users\iurii\.minikube\certs\C:\Users\iurii\.minikube\certs\cert.pem (1119 bytes)
I1224 10:05:24.039311   15264 certs.go:388] found cert: C:\Users\iurii\.minikube\certs\C:\Users\iurii\.minikube\certs\key.pem (1679 bytes)
I1224 10:05:24.041265   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1224 10:05:24.221039   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1224 10:05:24.714728   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1224 10:05:25.076093   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1224 10:05:25.253677   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1224 10:05:25.371549   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1224 10:05:25.570353   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1224 10:05:25.730758   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1224 10:05:25.866019   15264 ssh_runner.go:362] scp C:\Users\iurii\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1224 10:05:25.983322   15264 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1224 10:05:26.166123   15264 ssh_runner.go:195] Run: openssl version
I1224 10:05:26.340989   15264 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1224 10:05:26.522212   15264 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1224 10:05:26.561545   15264 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Dec 19 09:23 /usr/share/ca-certificates/minikubeCA.pem
I1224 10:05:26.644092   15264 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1224 10:05:26.758850   15264 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1224 10:05:26.853212   15264 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.28.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.36@sha256:8debc1b6a335075c5f99bfbf131b4f5566f68c6500dc5991817832e55fcc9456 Memory:3000 CPUs:2 DiskSize:20000 VMDriver: Driver:virtualbox HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:true DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.59.102 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\iurii:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1224 10:05:26.909479   15264 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1224 10:05:27.135711   15264 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1224 10:05:27.204534   15264 kubeadm.go:411] found existing configuration files, will attempt cluster restart
I1224 10:05:27.204534   15264 kubeadm.go:627] restartCluster start
I1224 10:05:27.263943   15264 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1224 10:05:27.318122   15264 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1224 10:05:27.319942   15264 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in C:\Users\iurii\.kube\config
I1224 10:05:27.320355   15264 kubeconfig.go:146] "minikube" context is missing from C:\Users\iurii\.kube\config - will repair!
I1224 10:05:27.321549   15264 lock.go:35] WriteFile acquiring C:\Users\iurii\.kube\config: {Name:mk5b4f5a3f6148a36a9dcc0a2b675c383c6d703f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1224 10:05:27.420598   15264 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1224 10:05:27.487796   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:27.583443   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:27.702367   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:27.902574   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:27.976325   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:28.077685   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:28.104929   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:28.208738   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:28.349702   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:28.503244   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:28.612660   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:28.765967   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:28.902827   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:28.954786   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:29.009438   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:29.102867   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:29.151243   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:29.200905   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:29.303966   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:29.391064   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:29.531354   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:29.702763   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:29.799296   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:29.870664   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:29.903288   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:29.955342   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:30.008105   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:30.102465   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:30.156954   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:30.213374   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:30.303139   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:30.349044   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:30.399289   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:30.502860   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:30.581804   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:30.661517   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:30.704091   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:30.760443   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:30.814247   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:30.814247   15264 api_server.go:165] Checking apiserver status ...
I1224 10:05:30.897717   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1224 10:05:31.017973   15264 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1224 10:05:31.017973   15264 kubeadm.go:602] needs reconfigure: apiserver error: timed out waiting for the condition
I1224 10:05:31.017973   15264 kubeadm.go:1114] stopping kube-system containers ...
I1224 10:05:31.086869   15264 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1224 10:05:31.327426   15264 docker.go:444] Stopping containers: [a62c2bf9f30a 946b58680c9e db477a9ca71d 854acbd6394c 4cbde11898c7 a61a272f7616 7b94107c4cc9 7e0cabf1852f aee595a7d3f8 fba6e6fe0afc 3d468a05be6d 6035996a660d afb85612e4b9 6346360436cc 0d83ac1df18b 711a4954385d 7a052be6988c b3046baffdd5 f1fec7a5f33c e4c2390aae49 f13b9ce7dab8 5e4a727e9f87 3e3cc343fe61 a36b1039305c 2c9656ba3d61 35ff60c40b67 aadb4d9da110]
I1224 10:05:31.377907   15264 ssh_runner.go:195] Run: docker stop a62c2bf9f30a 946b58680c9e db477a9ca71d 854acbd6394c 4cbde11898c7 a61a272f7616 7b94107c4cc9 7e0cabf1852f aee595a7d3f8 fba6e6fe0afc 3d468a05be6d 6035996a660d afb85612e4b9 6346360436cc 0d83ac1df18b 711a4954385d 7a052be6988c b3046baffdd5 f1fec7a5f33c e4c2390aae49 f13b9ce7dab8 5e4a727e9f87 3e3cc343fe61 a36b1039305c 2c9656ba3d61 35ff60c40b67 aadb4d9da110
I1224 10:05:31.652351   15264 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1224 10:05:31.790842   15264 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1224 10:05:31.837641   15264 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1224 10:05:31.893342   15264 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1224 10:05:31.942402   15264 kubeadm.go:704] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1224 10:05:31.942402   15264 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1224 10:05:47.093640   15264 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (15.1512382s)
I1224 10:05:47.093640   15264 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1224 10:05:52.190737   15264 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (5.097097s)
I1224 10:05:52.190737   15264 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1224 10:05:53.118527   15264 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1224 10:05:53.484163   15264 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1224 10:05:53.889555   15264 api_server.go:51] waiting for apiserver process to appear ...
I1224 10:05:53.942743   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:54.646745   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:55.574635   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:56.084114   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:56.594409   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:57.077983   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:57.594435   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:58.057167   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:58.582690   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:59.063052   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:05:59.623275   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:00.082060   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:00.610364   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:01.127838   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:01.622909   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:02.069870   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:02.594677   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:03.063935   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:03.605949   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:04.079386   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:04.646164   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:05.614841   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:06.101618   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:06.598967   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:07.061780   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:07.598632   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:08.071918   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:08.586840   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:09.062725   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:09.627253   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:10.574659   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:11.126954   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:11.643770   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:12.064737   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:12.614160   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:13.065431   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:13.612770   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:14.082619   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:14.645655   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:15.627681   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:16.143331   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:16.614962   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:17.065456   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:17.640042   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:18.101528   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:18.624704   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:19.109204   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:19.658974   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:20.653369   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:21.150855   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:21.642576   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:22.634015   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:23.112790   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:23.661040   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:24.099457   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:24.669749   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:25.649318   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:26.135985   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:26.612981   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:27.099204   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:27.637063   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:28.090687   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:28.623604   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:29.098094   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:29.618077   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:31.111306   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:32.090468   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:32.631040   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:33.076430   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:34.068415   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:34.655221   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:35.635021   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:36.124090   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:36.618899   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:37.629143   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:38.086979   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:39.059136   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:39.685048   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:40.631957   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:41.129482   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:41.647880   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:42.104525   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:42.636402   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:43.082814   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:43.629016   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:06:43.912503   15264 api_server.go:71] duration metric: took 50.0229485s to wait for apiserver process to appear ...
I1224 10:06:43.912503   15264 api_server.go:87] waiting for apiserver healthz status ...
I1224 10:06:43.912503   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:06:45.940174   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": dial tcp 192.168.59.102:8443: connectex: No connection could be made because the target machine actively refused it.
I1224 10:06:46.440635   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:06:48.457661   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": dial tcp 192.168.59.102:8443: connectex: No connection could be made because the target machine actively refused it.
I1224 10:06:48.942010   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:06:50.967366   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": dial tcp 192.168.59.102:8443: connectex: No connection could be made because the target machine actively refused it.
I1224 10:06:51.440598   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:06:53.452176   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": dial tcp 192.168.59.102:8443: connectex: No connection could be made because the target machine actively refused it.
I1224 10:06:53.940580   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:06:55.962951   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": dial tcp 192.168.59.102:8443: connectex: No connection could be made because the target machine actively refused it.
I1224 10:06:56.441593   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:06:58.458311   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": dial tcp 192.168.59.102:8443: connectex: No connection could be made because the target machine actively refused it.
I1224 10:06:58.941908   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:03.943422   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1224 10:07:04.440194   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:09.447112   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1224 10:07:09.940450   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:14.942088   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1224 10:07:14.944711   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:19.946081   15264 api_server.go:268] stopped: https://192.168.59.102:8443/healthz: Get "https://192.168.59.102:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1224 10:07:20.440328   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:22.466889   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1224 10:07:22.466889   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1224 10:07:22.941723   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:23.257565   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W1224 10:07:23.257814   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I1224 10:07:23.440513   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:23.580753   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1224 10:07:23.580753   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1224 10:07:23.942108   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:24.087451   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1224 10:07:24.088214   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1224 10:07:24.444798   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:24.550109   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1224 10:07:24.550109   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1224 10:07:24.940517   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:25.000834   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1224 10:07:25.000834   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1224 10:07:25.440802   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:25.659225   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1224 10:07:25.659225   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1224 10:07:25.942162   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:26.022082   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1224 10:07:26.022082   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1224 10:07:26.441125   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:26.696633   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1224 10:07:26.696633   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1224 10:07:26.940332   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:27.026472   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1224 10:07:27.026472   15264 api_server.go:102] status: https://192.168.59.102:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1224 10:07:27.440343   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:07:27.481906   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 200:
ok
I1224 10:07:27.581469   15264 api_server.go:140] control plane version: v1.25.3
I1224 10:07:27.581469   15264 api_server.go:130] duration metric: took 43.6689662s to wait for apiserver health ...
I1224 10:07:27.581469   15264 cni.go:95] Creating CNI manager for ""
I1224 10:07:27.581469   15264 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1224 10:07:27.607474   15264 system_pods.go:43] waiting for kube-system pods to appear ...
I1224 10:07:27.826488   15264 system_pods.go:59] 8 kube-system pods found
I1224 10:07:27.826488   15264 system_pods.go:61] "coredns-565d847f94-rxnrk" [e7a9a3d2-43ec-426d-93c7-b993781ff52d] Running
I1224 10:07:27.826488   15264 system_pods.go:61] "etcd-minikube" [0b5489b2-6c35-48a2-b2c7-abd466bb841a] Running
I1224 10:07:27.826488   15264 system_pods.go:61] "kube-apiserver-minikube" [847b461f-1002-4f35-bc12-53afaf72109d] Running
I1224 10:07:27.826488   15264 system_pods.go:61] "kube-controller-manager-minikube" [1df71cb3-5428-49d6-8853-addfac086f94] Running
I1224 10:07:27.826488   15264 system_pods.go:61] "kube-proxy-7vwgl" [71300a0f-a2d6-415d-a664-b5d010616b98] Running
I1224 10:07:27.826488   15264 system_pods.go:61] "kube-scheduler-minikube" [d06febf3-732e-4ebb-bbf9-f72477e3d314] Running
I1224 10:07:27.826488   15264 system_pods.go:61] "metrics-server-769cd898cd-sfqqh" [1d7b9542-44bf-4b77-8aec-a96ddc203ada] Running
I1224 10:07:27.826488   15264 system_pods.go:61] "storage-provisioner" [25914925-76c5-42cd-b49b-c270c6a88784] Running
I1224 10:07:27.826488   15264 system_pods.go:74] duration metric: took 219.0143ms to wait for pod list to return data ...
I1224 10:07:27.826488   15264 node_conditions.go:102] verifying NodePressure condition ...
I1224 10:07:27.843628   15264 node_conditions.go:122] node storage ephemeral capacity is 17784752Ki
I1224 10:07:27.846588   15264 node_conditions.go:123] node cpu capacity is 2
I1224 10:07:27.859171   15264 node_conditions.go:105] duration metric: took 32.6826ms to run NodePressure ...
I1224 10:07:27.859171   15264 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1224 10:07:30.333502   15264 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (2.4743305s)
I1224 10:07:30.333502   15264 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1224 10:07:30.433760   15264 ops.go:34] apiserver oom_adj: -16
I1224 10:07:30.433760   15264 kubeadm.go:631] restartCluster took 2m3.2292254s
I1224 10:07:30.433760   15264 kubeadm.go:398] StartCluster complete in 2m3.5805471s
I1224 10:07:30.433760   15264 settings.go:142] acquiring lock: {Name:mk9ae9548a0c5bc388d97cee2ba60ba6decfc6b0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1224 10:07:30.433985   15264 settings.go:150] Updating kubeconfig:  C:\Users\iurii\.kube\config
I1224 10:07:30.436515   15264 lock.go:35] WriteFile acquiring C:\Users\iurii\.kube\config: {Name:mk5b4f5a3f6148a36a9dcc0a2b675c383c6d703f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1224 10:07:30.652108   15264 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1224 10:07:30.671801   15264 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1224 10:07:30.686985   15264 global.go:111] Querying for installed drivers using PATH=C:\Program Files\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\Microsoft SQL Server\100\Tools\Binn\;C:\Program Files\Microsoft SQL Server\100\Tools\Binn\;C:\Program Files\Microsoft SQL Server\100\DTS\Binn\;C:\Program Files (x86)\Microsoft SQL Server\100\Tools\Binn\VSShell\Common7\IDE\;C:\Program Files (x86)\Microsoft SQL Server\100\DTS\Binn\;C:\Program Files (x86)\TerraLink\\php5;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Git\cmd;C:\Program Files\PostgreSQL\12\bin;C:\Users\iurii\AppData\Local\Android\Sdk\bin;C:\Users\iurii\AppData\Local\Android\Sdk\tools;C:\Users\iurii\AppData\Local\Android\Sdk\platform-tools;C:\Program Files\Docker\Docker\resources\bin;D:\Software\K8s;C:\Program Files\Kubernetes\Minikube;C:\Users\iurii\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk-11.0.11\bin;C:\Program Files\JetBrains\IntelliJ IDEA 2019.1.3\bin;;C:\apache-maven-3.6.3\bin;C:\Program Files\heroku\bin;C:\Program Files\Apache Software Foundation\Tomcat 9.0\lib\servlet-api.jar;C:\Program Files\Apache Software Foundation\Tomcat 9.0\bin;C:\Program Files\Apache Software Foundation\Tomcat 9.0\lib\servlet-api.jar;D:\GeekBrains\_software\_JAVA\WildFly-JakartaEE\wildfly-21.0.0.Final\bin;C:\Program Files\PostgreSQL\12\bin;;C:\Program Files\liquibase
I1224 10:07:30.689909   15264 config.go:180] Loaded profile config "minikube": Driver=virtualbox, ContainerRuntime=docker, KubernetesVersion=v1.25.3
I1224 10:07:30.652402   15264 addons.go:486] enableAddons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I1224 10:07:30.706504   15264 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1224 10:07:30.706504   15264 addons.go:227] Setting addon storage-provisioner=true in "minikube"
W1224 10:07:30.706504   15264 addons.go:236] addon storage-provisioner should already be in state true
I1224 10:07:30.707485   15264 host.go:66] Checking if "minikube" exists ...
I1224 10:07:30.709439   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:07:30.711391   15264 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1224 10:07:30.712369   15264 addons.go:65] Setting metrics-server=true in profile "minikube"
I1224 10:07:30.712369   15264 addons.go:227] Setting addon metrics-server=true in "minikube"
W1224 10:07:30.712369   15264 addons.go:236] addon metrics-server should already be in state true
I1224 10:07:30.712369   15264 host.go:66] Checking if "minikube" exists ...
I1224 10:07:30.715301   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:07:30.722284   15264 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1224 10:07:30.724240   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:07:30.772089   15264 addons.go:65] Setting dashboard=true in profile "minikube"
I1224 10:07:30.772089   15264 addons.go:227] Setting addon dashboard=true in "minikube"
W1224 10:07:30.772089   15264 addons.go:236] addon dashboard should already be in state true
I1224 10:07:30.773082   15264 host.go:66] Checking if "minikube" exists ...
I1224 10:07:30.775023   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:07:31.757395   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:07:31.757395   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:07:31.759352   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:07:31.759352   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:07:31.767408   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:07:31.767408   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:07:31.779142   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:07:31.779142   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:07:32.060715   15264 out.go:177]   - Используется образ k8s.gcr.io/metrics-server/metrics-server:v0.6.1
I1224 10:07:32.136866   15264 out.go:177]   - Используется образ docker.io/kubernetesui/dashboard:v2.7.0
I1224 10:07:32.270164   15264 out.go:177]   - Используется образ gcr.io/k8s-minikube/storage-provisioner:v5
I1224 10:07:32.380929   15264 addons.go:419] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I1224 10:07:32.380929   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I1224 10:07:32.551152   15264 addons.go:419] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1224 10:07:32.551152   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1224 10:07:32.551152   15264 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57780 SSHKeyPath:C:\Users\iurii\.minikube\machines\minikube\id_rsa Username:docker}
I1224 10:07:32.342857   15264 addons.go:227] Setting addon default-storageclass=true in "minikube"
W1224 10:07:32.553108   15264 addons.go:236] addon default-storageclass should already be in state true
I1224 10:07:32.554083   15264 host.go:66] Checking if "minikube" exists ...
I1224 10:07:32.557015   15264 main.go:134] libmachine: COMMAND: C:\Program Files\Oracle\VirtualBox\VBoxManage.exe showvminfo minikube --machinereadable
I1224 10:07:32.381905   15264 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57780 SSHKeyPath:C:\Users\iurii\.minikube\machines\minikube\id_rsa Username:docker}
I1224 10:07:32.855060   15264 out.go:177]   - Используется образ docker.io/kubernetesui/metrics-scraper:v1.0.8
I1224 10:07:32.957533   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1224 10:07:32.957533   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1224 10:07:32.958505   15264 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57780 SSHKeyPath:C:\Users\iurii\.minikube\machines\minikube\id_rsa Username:docker}
I1224 10:07:32.904866   15264 main.go:134] libmachine: STDOUT:
{
name="minikube"
Encryption:     disabled
groups="/"
ostype="Linux 2.6 / 3.x / 4.x / 5.x (64-bit)"
UUID="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
CfgFile="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.vbox"
SnapFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Snapshots"
LogFldr="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\Logs"
hardwareuuid="bcc3a64a-ab30-49db-8db4-52dc266a2cea"
memory=3000
pagefusion="off"
vram=8
cpuexecutioncap=100
hpet="on"
cpu-profile="host"
chipset="piix3"
firmware="BIOS"
cpus=2
pae="on"
longmode="on"
triplefaultreset="off"
apic="on"
x2apic="off"
nested-hw-virt="off"
cpuid-portability-level=0
bootmenu="disabled"
boot1="dvd"
boot2="dvd"
boot3="disk"
boot4="none"
acpi="on"
ioapic="on"
biosapic="apic"
biossystemtimeoffset=0
BIOS NVRAM File="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube.nvram"
rtcuseutc="on"
hwvirtex="on"
nestedpaging="on"
largepages="on"
vtxvpid="on"
vtxux="on"
virtvmsavevmload="on"
iommu="none"
paravirtprovider="default"
effparavirtprovider="kvm"
VMState="running"
VMStateChangeTime="2022-12-24T07:03:33.390000000"
graphicscontroller="vboxvga"
monitorcount=1
accelerate3d="off"
accelerate2dvideo="off"
teleporterenabled="off"
teleporterport=0
teleporteraddress=""
teleporterpassword=""
tracing-enabled="off"
tracing-allow-vm-access="off"
tracing-config=""
autostart-enabled="off"
autostart-delay=0
defaultfrontend=""
vmprocpriority="default"
storagecontrollername0="SATA"
storagecontrollertype0="IntelAhci"
storagecontrollerinstance0="0"
storagecontrollermaxportcount0="30"
storagecontrollerportcount0="30"
storagecontrollerbootable0="on"
"SATA-0-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\boot2docker.iso"
"SATA-ImageUUID-0-0"="3d4c0145-820e-43e0-a168-f035a0312d97"
"SATA-tempeject-0-0"="off"
"SATA-IsEjected-0-0"="off"
"SATA-hot-pluggable-0-0"="off"
"SATA-nonrotational-0-0"="off"
"SATA-discard-0-0"="off"
"SATA-1-0"="C:\\Users\\iurii\\.minikube\\machines\\minikube\\disk.vmdk"
"SATA-ImageUUID-1-0"="d4ac3367-08e6-4356-852c-06ed2ed6a9d5"
"SATA-hot-pluggable-1-0"="off"
"SATA-nonrotational-1-0"="off"
"SATA-discard-1-0"="off"
"SATA-2-0"="none"
"SATA-3-0"="none"
"SATA-4-0"="none"
"SATA-5-0"="none"
"SATA-6-0"="none"
"SATA-7-0"="none"
"SATA-8-0"="none"
"SATA-9-0"="none"
"SATA-10-0"="none"
"SATA-11-0"="none"
"SATA-12-0"="none"
"SATA-13-0"="none"
"SATA-14-0"="none"
"SATA-15-0"="none"
"SATA-16-0"="none"
"SATA-17-0"="none"
"SATA-18-0"="none"
"SATA-19-0"="none"
"SATA-20-0"="none"
"SATA-21-0"="none"
"SATA-22-0"="none"
"SATA-23-0"="none"
"SATA-24-0"="none"
"SATA-25-0"="none"
"SATA-26-0"="none"
"SATA-27-0"="none"
"SATA-28-0"="none"
"SATA-29-0"="none"
natnet1="nat"
macaddress1="08002703EAA5"
cableconnected1="on"
nic1="nat"
nictype1="virtio"
nicspeed1="0"
mtu="0"
sockSnd="64"
sockRcv="64"
tcpWndSnd="64"
tcpWndRcv="64"
Forwarding(0)="ssh,tcp,127.0.0.1,57780,,22"
hostonlyadapter2="VirtualBox Host-Only Ethernet Adapter #2"
macaddress2="08002772F78F"
cableconnected2="on"
nic2="hostonly"
nictype2="virtio"
nicspeed2="0"
nic3="none"
nic4="none"
nic5="none"
nic6="none"
nic7="none"
nic8="none"
hidpointing="ps2mouse"
hidkeyboard="ps2kbd"
uart1="off"
uart2="off"
uart3="off"
uart4="off"
lpt1="off"
lpt2="off"
audio="default"
audio_out="off"
audio_in="off"
clipboard="disabled"
draganddrop="disabled"
SessionName="headless"
VideoMode="720,400,0"@0,0 1
vrde="off"
usb="off"
ehci="off"
xhci="off"
SharedFolderNameMachineMapping1="c/Users"
SharedFolderPathMachineMapping1="\\\\?\\c:\\Users"
VRDEActiveConnection="off"
VRDEClients==0
recording_enabled="off"
recording_screens=1
 rec_screen0
rec_screen_enabled="on"
rec_screen_id=0
rec_screen_video_enabled="on"
rec_screen_audio_enabled="off"
rec_screen_dest="File"
rec_screen_dest_filename="C:\\Users\\iurii\\.minikube\\machines\\minikube\\minikube\\minikube-screen0.webm"
rec_screen_opts="vc_enabled=true,ac_enabled=false,ac_profile=med"
rec_screen_video_res_xy="1024x768"
rec_screen_video_rate_kbps=512
rec_screen_video_fps=25
GuestMemoryBalloon=0
GuestOSType="Linux26_64"
GuestAdditionsRunLevel=2
GuestAdditionsVersion="6.0.0 r127566"
GuestAdditionsFacility_VirtualBox Base Driver=50,1671865480104
GuestAdditionsFacility_VirtualBox System Service=50,1671865482527
GuestAdditionsFacility_Seamless Mode=0,1671865480045
GuestAdditionsFacility_Graphics Mode=0,1671865480045
}
I1224 10:07:32.959490   15264 main.go:134] libmachine: STDERR:
{
}
I1224 10:07:32.959490   15264 addons.go:419] installing /etc/kubernetes/addons/storageclass.yaml
I1224 10:07:32.959490   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1224 10:07:32.960472   15264 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57780 SSHKeyPath:C:\Users\iurii\.minikube\machines\minikube\id_rsa Username:docker}
I1224 10:07:33.387758   15264 docker.go:137] docker version: linux-20.10.21
I1224 10:07:33.486626   15264 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1224 10:07:36.115555   15264 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1224 10:07:36.362933   15264 addons.go:419] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I1224 10:07:36.362933   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1902 bytes)
I1224 10:07:37.916690   15264 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1224 10:07:39.439226   15264 addons.go:419] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I1224 10:07:39.458757   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I1224 10:07:40.207239   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1224 10:07:40.207239   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1224 10:07:43.630060   15264 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (10.1434348s)
I1224 10:07:43.786236   15264 info.go:266] docker info: {ID:RDPR:GZR7:FFJ3:FVVU:JDS6:ENLG:YFKP:DRL4:GXOJ:355B:ZCBJ:DMEY Containers:4 ContainersRunning:0 ContainersPaused:0 ContainersStopped:4 Images:14 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:true NFd:45 OomKillDisable:true NGoroutines:47 SystemTime:2022-12-24 07:07:34.8302483 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:4.19.104-microsoft-standard OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:9946824704 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1c90a442489720eec95342e1789ee8a5e1b9536f Expected:1c90a442489720eec95342e1789ee8a5e1b9536f} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.3] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.13] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1224 10:07:43.786236   15264 global.go:119] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1224 10:07:43.880765   15264 addons.go:419] installing /etc/kubernetes/addons/metrics-server-service.yaml
I1224 10:07:43.880765   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I1224 10:07:43.974917   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1224 10:07:43.974917   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1224 10:07:46.854938   15264 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I1224 10:07:48.837186   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1224 10:07:48.837186   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1224 10:07:51.907156   15264 global.go:119] hyperv default: true priority: 8, state: {Installed:false Healthy:false Running:true NeedsImprovement:false Error:C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @(Get-CimInstance Win32_ComputerSystem).HypervisorPresent failed:
 
C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -NoProfile -NonInteractive @(Get-Wmiobject Win32_ComputerSystem).HypervisorPresent failed:
  Reason: Fix:Start PowerShell as an Administrator Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperv/ Version:}
I1224 10:07:52.069287   15264 global.go:119] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1224 10:07:52.246483   15264 global.go:119] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1224 10:07:52.246483   15264 global.go:119] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1224 10:07:52.364198   15264 virtualbox.go:136] virtual box version: 7.0.4r154605
I1224 10:07:52.364198   15264 global.go:119] virtualbox default: true priority: 6, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:7.0.4r154605
}
I1224 10:07:52.502317   15264 global.go:119] vmware default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "docker-machine-driver-vmware": executable file not found in %!P(MISSING)ATH%!R(MISSING)eason: Fix:Install docker-machine-driver-vmware Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1224 10:07:52.502877   15264 start.go:212] Will wait 6m0s for node &{Name: IP:192.168.59.102 Port:8443 KubernetesVersion:v1.25.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1224 10:07:52.552734   15264 out.go:177] * Компоненты Kubernetes проверяются ...
I1224 10:07:52.738246   15264 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1224 10:07:54.142123   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1224 10:07:54.142123   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4298 bytes)
I1224 10:07:56.704183   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-role.yaml
I1224 10:07:56.704183   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1224 10:07:58.351397   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1224 10:07:58.351397   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1224 10:07:58.905991   15264 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (28.2341893s)
I1224 10:07:58.906965   15264 start.go:806] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1224 10:07:59.707404   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1224 10:07:59.707404   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1224 10:08:01.887002   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1224 10:08:01.887002   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1224 10:08:02.925035   15264 addons.go:419] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1224 10:08:02.925035   15264 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1224 10:08:05.550659   15264 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1224 10:08:23.677846   15264 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (45.7611559s)
I1224 10:08:23.683709   15264 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (47.568154s)
I1224 10:08:23.779397   15264 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (36.9244585s)
I1224 10:08:23.780369   15264 addons.go:457] Verifying addon metrics-server=true in "minikube"
I1224 10:08:23.780369   15264 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (31.0411511s)
I1224 10:08:23.780369   15264 api_server.go:51] waiting for apiserver process to appear ...
I1224 10:08:23.850677   15264 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1224 10:08:26.321119   15264 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (20.7704602s)
I1224 10:08:26.358105   15264 out.go:177] * Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I1224 10:08:26.322098   15264 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.4704417s)
I1224 10:08:26.362003   15264 api_server.go:71] duration metric: took 33.859126s to wait for apiserver process to appear ...
I1224 10:08:26.362003   15264 api_server.go:87] waiting for apiserver healthz status ...
I1224 10:08:26.362003   15264 api_server.go:252] Checking apiserver healthz at https://192.168.59.102:8443/healthz ...
I1224 10:08:26.454141   15264 out.go:177] * Включенные дополнения: default-storageclass, storage-provisioner, metrics-server, dashboard
I1224 10:08:26.509804   15264 addons.go:488] enableAddons completed in 55.857402s
I1224 10:08:26.809580   15264 api_server.go:278] https://192.168.59.102:8443/healthz returned 200:
ok
I1224 10:08:26.861546   15264 api_server.go:140] control plane version: v1.25.3
I1224 10:08:26.861546   15264 api_server.go:130] duration metric: took 499.5429ms to wait for apiserver health ...
I1224 10:08:26.861546   15264 system_pods.go:43] waiting for kube-system pods to appear ...
I1224 10:08:26.995516   15264 system_pods.go:59] 8 kube-system pods found
I1224 10:08:26.995516   15264 system_pods.go:61] "coredns-565d847f94-rxnrk" [e7a9a3d2-43ec-426d-93c7-b993781ff52d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1224 10:08:26.995516   15264 system_pods.go:61] "etcd-minikube" [0b5489b2-6c35-48a2-b2c7-abd466bb841a] Running
I1224 10:08:26.995516   15264 system_pods.go:61] "kube-apiserver-minikube" [847b461f-1002-4f35-bc12-53afaf72109d] Running
I1224 10:08:26.995516   15264 system_pods.go:61] "kube-controller-manager-minikube" [1df71cb3-5428-49d6-8853-addfac086f94] Running
I1224 10:08:26.995516   15264 system_pods.go:61] "kube-proxy-7vwgl" [71300a0f-a2d6-415d-a664-b5d010616b98] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1224 10:08:26.995516   15264 system_pods.go:61] "kube-scheduler-minikube" [d06febf3-732e-4ebb-bbf9-f72477e3d314] Running
I1224 10:08:26.995516   15264 system_pods.go:61] "metrics-server-769cd898cd-sfqqh" [1d7b9542-44bf-4b77-8aec-a96ddc203ada] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I1224 10:08:26.995516   15264 system_pods.go:61] "storage-provisioner" [25914925-76c5-42cd-b49b-c270c6a88784] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1224 10:08:26.995516   15264 system_pods.go:74] duration metric: took 133.9702ms to wait for pod list to return data ...
I1224 10:08:26.995516   15264 kubeadm.go:573] duration metric: took 34.4926391s to wait for : map[apiserver:true system_pods:true] ...
I1224 10:08:26.995516   15264 node_conditions.go:102] verifying NodePressure condition ...
I1224 10:08:27.300746   15264 node_conditions.go:122] node storage ephemeral capacity is 17784752Ki
I1224 10:08:27.301261   15264 node_conditions.go:123] node cpu capacity is 2
I1224 10:08:27.301261   15264 node_conditions.go:105] duration metric: took 305.745ms to run NodePressure ...
I1224 10:08:27.301350   15264 start.go:217] waiting for startup goroutines ...
I1224 10:08:27.385330   15264 ssh_runner.go:195] Run: rm -f paused
I1224 10:08:28.099082   15264 start.go:506] kubectl: 1.25.2, cluster: 1.25.3 (minor skew: 0)
I1224 10:08:28.180923   15264 out.go:177] * Готово! kubectl настроен для использования кластера "minikube" и "default" пространства имён по умолчанию

* 
* ==> Docker <==
* -- Journal begins at Sat 2022-12-24 07:04:37 UTC, ends at Sat 2022-12-24 11:27:15 UTC. --
Dec 24 10:19:30 minikube dockerd[948]: time="2022-12-24T10:19:30.703757678Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 24 10:19:30 minikube dockerd[948]: time="2022-12-24T10:19:30.703781299Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 24 10:19:30 minikube dockerd[948]: time="2022-12-24T10:19:30.706417367Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/4fe6640912be9dc34f0fca111f48a12e07a52753346e45467389489e22bede95 pid=97396 runtime=io.containerd.runc.v2
Dec 24 10:19:46 minikube dockerd[942]: time="2022-12-24T10:19:46.733783252Z" level=warning msg="reference for unknown type: " digest="sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660" remote="k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660"
Dec 24 10:20:21 minikube dockerd[948]: time="2022-12-24T10:20:21.849385909Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 24 10:20:21 minikube dockerd[948]: time="2022-12-24T10:20:21.863126863Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 24 10:20:21 minikube dockerd[948]: time="2022-12-24T10:20:21.863180028Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 24 10:20:21 minikube dockerd[948]: time="2022-12-24T10:20:21.863565810Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/fb80bc64394a886d149c91b8543ab393648c05af00a5df4cba086379fc32fe6b pid=98042 runtime=io.containerd.runc.v2
Dec 24 10:20:22 minikube dockerd[948]: time="2022-12-24T10:20:22.280618574Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 24 10:20:22 minikube dockerd[948]: time="2022-12-24T10:20:22.281601953Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 24 10:20:22 minikube dockerd[948]: time="2022-12-24T10:20:22.281657417Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 24 10:20:22 minikube dockerd[948]: time="2022-12-24T10:20:22.282586673Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/d0a1e41a9f497b09be794d598178ceb3ca8aa56d6d89823af6985d12b6389e71 pid=98077 runtime=io.containerd.runc.v2
Dec 24 10:20:25 minikube dockerd[948]: time="2022-12-24T10:20:25.695644841Z" level=info msg="shim disconnected" id=fb80bc64394a886d149c91b8543ab393648c05af00a5df4cba086379fc32fe6b
Dec 24 10:20:25 minikube dockerd[948]: time="2022-12-24T10:20:25.698905245Z" level=warning msg="cleaning up after shim disconnected" id=fb80bc64394a886d149c91b8543ab393648c05af00a5df4cba086379fc32fe6b namespace=moby
Dec 24 10:20:25 minikube dockerd[948]: time="2022-12-24T10:20:25.698954306Z" level=info msg="cleaning up dead shim"
Dec 24 10:20:25 minikube dockerd[948]: time="2022-12-24T10:20:25.716906976Z" level=warning msg="cleanup warnings time=\"2022-12-24T10:20:25Z\" level=info msg=\"starting signal loop\" namespace=moby pid=98143 runtime=io.containerd.runc.v2\n"
Dec 24 10:20:25 minikube dockerd[942]: time="2022-12-24T10:20:25.839739127Z" level=info msg="ignoring event" container=fb80bc64394a886d149c91b8543ab393648c05af00a5df4cba086379fc32fe6b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 24 10:20:26 minikube dockerd[948]: time="2022-12-24T10:20:26.841467014Z" level=info msg="shim disconnected" id=d0a1e41a9f497b09be794d598178ceb3ca8aa56d6d89823af6985d12b6389e71
Dec 24 10:20:26 minikube dockerd[948]: time="2022-12-24T10:20:26.842456517Z" level=warning msg="cleaning up after shim disconnected" id=d0a1e41a9f497b09be794d598178ceb3ca8aa56d6d89823af6985d12b6389e71 namespace=moby
Dec 24 10:20:26 minikube dockerd[948]: time="2022-12-24T10:20:26.842718699Z" level=info msg="cleaning up dead shim"
Dec 24 10:20:26 minikube dockerd[942]: time="2022-12-24T10:20:26.891649378Z" level=info msg="ignoring event" container=d0a1e41a9f497b09be794d598178ceb3ca8aa56d6d89823af6985d12b6389e71 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 24 10:20:27 minikube dockerd[948]: time="2022-12-24T10:20:27.252855465Z" level=warning msg="cleanup warnings time=\"2022-12-24T10:20:27Z\" level=info msg=\"starting signal loop\" namespace=moby pid=98164 runtime=io.containerd.runc.v2\n"
Dec 24 10:20:27 minikube dockerd[948]: time="2022-12-24T10:20:27.800104263Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 24 10:20:27 minikube dockerd[948]: time="2022-12-24T10:20:27.802253865Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 24 10:20:27 minikube dockerd[948]: time="2022-12-24T10:20:27.802290408Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 24 10:20:27 minikube dockerd[948]: time="2022-12-24T10:20:27.807325643Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/8282809ab2c5aea1274402c8f64dc3ca2ad77d35b60751f87c39b5c039ce8c12 pid=98185 runtime=io.containerd.runc.v2
Dec 24 10:20:32 minikube dockerd[942]: time="2022-12-24T10:20:32.101195633Z" level=info msg="ignoring event" container=8282809ab2c5aea1274402c8f64dc3ca2ad77d35b60751f87c39b5c039ce8c12 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 24 10:20:32 minikube dockerd[948]: time="2022-12-24T10:20:32.102457435Z" level=info msg="shim disconnected" id=8282809ab2c5aea1274402c8f64dc3ca2ad77d35b60751f87c39b5c039ce8c12
Dec 24 10:20:32 minikube dockerd[948]: time="2022-12-24T10:20:32.102532116Z" level=warning msg="cleaning up after shim disconnected" id=8282809ab2c5aea1274402c8f64dc3ca2ad77d35b60751f87c39b5c039ce8c12 namespace=moby
Dec 24 10:20:32 minikube dockerd[948]: time="2022-12-24T10:20:32.102552047Z" level=info msg="cleaning up dead shim"
Dec 24 10:20:32 minikube dockerd[942]: time="2022-12-24T10:20:32.288065618Z" level=info msg="ignoring event" container=4fe6640912be9dc34f0fca111f48a12e07a52753346e45467389489e22bede95 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 24 10:20:32 minikube dockerd[948]: time="2022-12-24T10:20:32.295197231Z" level=info msg="shim disconnected" id=4fe6640912be9dc34f0fca111f48a12e07a52753346e45467389489e22bede95
Dec 24 10:20:32 minikube dockerd[948]: time="2022-12-24T10:20:32.295744626Z" level=warning msg="cleaning up after shim disconnected" id=4fe6640912be9dc34f0fca111f48a12e07a52753346e45467389489e22bede95 namespace=moby
Dec 24 10:20:32 minikube dockerd[948]: time="2022-12-24T10:20:32.295776213Z" level=info msg="cleaning up dead shim"
Dec 24 10:20:32 minikube dockerd[948]: time="2022-12-24T10:20:32.427160384Z" level=warning msg="cleanup warnings time=\"2022-12-24T10:20:32Z\" level=info msg=\"starting signal loop\" namespace=moby pid=98261 runtime=io.containerd.runc.v2\n"
Dec 24 10:20:32 minikube dockerd[948]: time="2022-12-24T10:20:32.612952984Z" level=warning msg="cleanup warnings time=\"2022-12-24T10:20:32Z\" level=info msg=\"starting signal loop\" namespace=moby pid=98275 runtime=io.containerd.runc.v2\n"
Dec 24 10:20:36 minikube dockerd[948]: time="2022-12-24T10:20:36.650420315Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 24 10:20:36 minikube dockerd[948]: time="2022-12-24T10:20:36.652792590Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 24 10:20:36 minikube dockerd[948]: time="2022-12-24T10:20:36.652919318Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 24 10:20:36 minikube dockerd[948]: time="2022-12-24T10:20:36.654392518Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/428072de26b5c08998000d76ab3def3fd4f47bd986b01510a24ff4676f34f550 pid=98500 runtime=io.containerd.runc.v2
Dec 24 10:20:41 minikube dockerd[942]: time="2022-12-24T10:20:41.066885196Z" level=warning msg="reference for unknown type: " digest="sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8" remote="k8s.gcr.io/ingress-nginx/controller@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Dec 24 10:20:45 minikube dockerd[948]: time="2022-12-24T10:20:45.544582792Z" level=info msg="shim disconnected" id=fc854c0904a1fb2a9c99e1edf6d2a552169d57f3a75e0c57299e06a95f4761f5
Dec 24 10:20:45 minikube dockerd[948]: time="2022-12-24T10:20:45.556806522Z" level=warning msg="cleaning up after shim disconnected" id=fc854c0904a1fb2a9c99e1edf6d2a552169d57f3a75e0c57299e06a95f4761f5 namespace=moby
Dec 24 10:20:45 minikube dockerd[948]: time="2022-12-24T10:20:45.556866037Z" level=info msg="cleaning up dead shim"
Dec 24 10:20:45 minikube dockerd[942]: time="2022-12-24T10:20:45.836259805Z" level=info msg="ignoring event" container=fc854c0904a1fb2a9c99e1edf6d2a552169d57f3a75e0c57299e06a95f4761f5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 24 10:20:47 minikube dockerd[948]: time="2022-12-24T10:20:47.551522787Z" level=warning msg="cleanup warnings time=\"2022-12-24T10:20:45Z\" level=info msg=\"starting signal loop\" namespace=moby pid=98584 runtime=io.containerd.runc.v2\n"
Dec 24 10:22:39 minikube dockerd[942]: time="2022-12-24T10:22:39.959649726Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 10:25:01 minikube dockerd[942]: time="2022-12-24T10:25:01.246705855Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 10:25:01 minikube dockerd[942]: time="2022-12-24T10:25:01.924654055Z" level=info msg="Layer sha256:b541d28bf3b491aeb424c61353c8c92476ecc2cd603a6c09ee5c2708f1a4b258 cleaned up"
Dec 24 10:27:32 minikube dockerd[942]: time="2022-12-24T10:27:32.399177594Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 10:30:13 minikube dockerd[942]: time="2022-12-24T10:30:13.214930807Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 10:33:37 minikube dockerd[942]: time="2022-12-24T10:33:37.677198649Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 10:38:23 minikube dockerd[942]: time="2022-12-24T10:38:23.436471770Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 10:45:31 minikube dockerd[942]: time="2022-12-24T10:45:31.162200610Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 10:52:36 minikube dockerd[942]: time="2022-12-24T10:52:36.008129298Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 10:59:37 minikube dockerd[942]: time="2022-12-24T10:59:37.950437756Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 11:06:51 minikube dockerd[942]: time="2022-12-24T11:06:51.478177168Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 11:13:55 minikube dockerd[942]: time="2022-12-24T11:13:55.051330070Z" level=error msg="Not continuing with pull after error: context canceled"
Dec 24 11:13:55 minikube dockerd[942]: time="2022-12-24T11:13:55.624586080Z" level=info msg="Layer sha256:b541d28bf3b491aeb424c61353c8c92476ecc2cd603a6c09ee5c2708f1a4b258 cleaned up"
Dec 24 11:21:00 minikube dockerd[942]: time="2022-12-24T11:21:00.846522351Z" level=error msg="Not continuing with pull after error: context canceled"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                        ATTEMPT             POD ID
8282809ab2c5a       c41e9fcadf5a2                                                                                                           About an hour ago   Exited              patch                       1                   fc854c0904a1f
d0a1e41a9f497       k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660   About an hour ago   Exited              create                      0                   4fe6640912be9
9a256805e5987       4d9b526b5e554                                                                                                           2 hours ago         Running             users-app                   0                   8950a534d4da7
36eec832185e4       4d9b526b5e554                                                                                                           2 hours ago         Running             users-app                   0                   aa5279662c742
a6fe6cb83c724       6e38f40d628db                                                                                                           4 hours ago         Running             storage-provisioner         22                  6d34a1c166410
7690832fbc15b       e57a417f15d36                                                                                                           4 hours ago         Running             metrics-server              8                   d2d58eeace59b
9c37402a2a5f6       07655ddf2eebe                                                                                                           4 hours ago         Running             kubernetes-dashboard        14                  3eabc848069ce
f44ad2cbb158a       115053965e86b                                                                                                           4 hours ago         Running             dashboard-metrics-scraper   4                   8c65ebb4fba0a
c9f179e0cb001       3dfb9f0162b06                                                                                                           4 hours ago         Running             users-pg-db                 3                   b787489037830
85f4126416f15       5185b96f0becf                                                                                                           4 hours ago         Running             coredns                     4                   3f1dd75154023
2ad36f9b307ff       3dfb9f0162b06                                                                                                           4 hours ago         Running             users-pg-db                 3                   a6a4b82e313f4
9fcd8e098d876       beaaf00edd38a                                                                                                           4 hours ago         Running             kube-proxy                  4                   fd69565dc15a8
33c691fe5213e       6039992312758                                                                                                           4 hours ago         Running             kube-controller-manager     9                   af5d62df59e98
ee0fe6950d4a3       a8a176a5d5d69                                                                                                           4 hours ago         Running             etcd                        4                   7db4d12328161
676a16d46a2b2       0346dbd74bcb9                                                                                                           4 hours ago         Running             kube-apiserver              10                  64f6d8e487afd
27b14bea83e8e       6d23ec0e8b87e                                                                                                           4 hours ago         Running             kube-scheduler              4                   f27e2ff7f7ad9
1111d9a540afa       6039992312758                                                                                                           4 hours ago         Exited              kube-controller-manager     8                   af5d62df59e98
4649ea3a10b39       3dfb9f0162b06                                                                                                           5 hours ago         Exited              users-pg-db                 2                   96fcbacefd59b
b05aa56388526       3dfb9f0162b06                                                                                                           5 hours ago         Exited              users-pg-db                 2                   75dfb68eebd04
577a681cbab9f       07655ddf2eebe                                                                                                           5 hours ago         Exited              kubernetes-dashboard        13                  bc956f4be92c8
db477a9ca71d4       5185b96f0becf                                                                                                           5 hours ago         Exited              coredns                     3                   aee595a7d3f8a
da96c1bb29dd0       115053965e86b                                                                                                           5 hours ago         Exited              dashboard-metrics-scraper   3                   7011221fe0047
6035996a660d4       a8a176a5d5d69                                                                                                           5 hours ago         Exited              etcd                        3                   711a4954385dc
6346360436cc6       0346dbd74bcb9                                                                                                           5 hours ago         Exited              kube-apiserver              9                   b3046baffdd52
0d83ac1df18bf       6d23ec0e8b87e                                                                                                           5 hours ago         Exited              kube-scheduler              3                   f1fec7a5f33c5

* 
* ==> coredns [85f4126416f1] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = ce5ff3ccd9a547cb87fa037b47bfefcf0242ccdccedade419b1cdf4161767373e4512784d241f572235dd6af1e8841eeeea92e663367352d92ff026f15feba54
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.088509528s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.006318696s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.006937001s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.014177336s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.245993015s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.172804844s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.079220408s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.04282015s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.041654486s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.025127413s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.105124461s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.088960185s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.116895211s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.041249608s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.394310468s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.01722325s

* 
* ==> coredns [db477a9ca71d] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = ce5ff3ccd9a547cb87fa037b47bfefcf0242ccdccedade419b1cdf4161767373e4512784d241f572235dd6af1e8841eeeea92e663367352d92ff026f15feba54
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.042111688s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.376214465s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.045774324s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.014768083s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.031921751s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.010143597s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.088920505s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=986b1ebd987211ed16f8cc10aed7d2c42fc8392f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_12_22T13_09_13_0700
                    minikube.k8s.io/version=v1.28.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 22 Dec 2022 10:08:38 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 24 Dec 2022 11:27:13 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 24 Dec 2022 11:26:58 +0000   Thu, 22 Dec 2022 12:51:47 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 24 Dec 2022 11:26:58 +0000   Thu, 22 Dec 2022 12:51:47 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 24 Dec 2022 11:26:58 +0000   Thu, 22 Dec 2022 12:51:47 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 24 Dec 2022 11:26:58 +0000   Thu, 22 Dec 2022 12:51:47 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.59.102
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             2972892Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             2972892Ki
  pods:               110
System Info:
  Machine ID:                 a7e1f626a8f040bc9351c0cd61140edd
  System UUID:                4aa6c3bc-30ab-db49-8db4-52dc266a2cea
  Boot ID:                    45b68434-1607-40f1-81e2-c388630a0ba5
  Kernel Version:             5.10.57
  OS Image:                   Buildroot 2021.02.12
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.20
  Kubelet Version:            v1.25.3
  Kube-Proxy Version:         v1.25.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (15 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     users-app-deployment-58b778899c-7nhhl        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         103m
  default                     users-app-deployment-58b778899c-pv7w9        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         103m
  default                     users-pg-db-deployment-bdc84b778-kj6dp       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         24h
  default                     users-pg-db-deployment-bdc84b778-tqbvd       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         24h
  ingress-nginx               ingress-nginx-controller-5959f988fd-vql45    100m (5%!)(MISSING)     0 (0%!)(MISSING)      90Mi (3%!)(MISSING)        0 (0%!)(MISSING)         68m
  kube-system                 coredns-565d847f94-rxnrk                     100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (2%!)(MISSING)        170Mi (5%!)(MISSING)     2d1h
  kube-system                 etcd-minikube                                100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (3%!)(MISSING)       0 (0%!)(MISSING)         2d1h
  kube-system                 kube-apiserver-minikube                      250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kube-system                 kube-controller-manager-minikube             200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kube-system                 kube-proxy-7vwgl                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kube-system                 kube-scheduler-minikube                      100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kube-system                 metrics-server-769cd898cd-sfqqh              100m (5%!)(MISSING)     0 (0%!)(MISSING)      200Mi (6%!)(MISSING)       0 (0%!)(MISSING)         2d1h
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kubernetes-dashboard        dashboard-metrics-scraper-b74747df5-fpkfk    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kubernetes-dashboard        kubernetes-dashboard-57bbdc5f89-hltbf        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                950m (47%!)(MISSING)   0 (0%!)(MISSING)
  memory             460Mi (15%!)(MISSING)  170Mi (5%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.000005]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
[  +0.000003] RIP: 0033:0x7fc514cb9a93
[  +0.000003] Code: 0d 00 f7 d8 64 89 02 48 c7 c0 ff ff ff ff eb bb 0f 1f 80 00 00 00 00 64 8b 04 25 18 00 00 00 85 c0 75 14 b8 01 00 00 00 0f 05 <48> 3d 00 f0 ff ff 77 55 c3 0f 1f 40 00 48 83 ec 28 48 89 54 24 18
[  +0.000001] RSP: 002b:00007ffc05e1ddf8 EFLAGS: 00000246 ORIG_RAX: 0000000000000001
[  +0.000003] RAX: ffffffffffffffda RBX: 00007ffc05e1de00 RCX: 00007fc514cb9a93
[  +0.000001] RDX: 0000000000000002 RSI: 00007ffc05e1de00 RDI: 0000000000000004
[  +0.000002] RBP: 00007ffc05e1de70 R08: 000000000000c1c2 R09: 77726f665f70692f
[  +0.000001] R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000
[  +0.000002] R13: 0000000000000004 R14: 0000000000000002 R15: 00007ffc05e1dee0
[  +0.000002] ---[ end trace 688883f998caf290 ]---
[  +0.001810] ------------[ cut here ]------------
[  +0.000002] netdevice: eth1: failed to disable LRO!
[  +0.000233] WARNING: CPU: 1 PID: 225 at net/core/dev.c:1694 dev_disable_lro+0xb2/0xf0
[  +0.000002] Modules linked in:
[  +0.000005] CPU: 1 PID: 225 Comm: systemd-sysctl Tainted: G        W         5.10.57 #1
[  +0.000001] Hardware name: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006
[  +0.000003] RIP: 0010:dev_disable_lro+0xb2/0xf0
[  +0.000003] Code: 2c 8b 74 14 be 25 00 00 00 48 89 df e8 c7 c2 b3 ff 48 85 c0 48 0f 44 eb 4c 89 e2 48 89 ee 48 c7 c7 88 19 2f 8b e8 13 65 2d 00 <0f> 0b e9 6c ff ff ff 80 3d 61 23 99 01 00 49 c7 c4 fa 9d 2c 8b 75
[  +0.000001] RSP: 0018:ffffac29c07dbd98 EFLAGS: 00010282
[  +0.000003] RAX: 0000000000000000 RBX: ffffa3cc3f16f000 RCX: c0000000ffffdfff
[  +0.000001] RDX: ffffac29c07dbbc0 RSI: 00000000ffffdfff RDI: ffffffff8c4987ac
[  +0.000001] RBP: ffffa3cc3f16f000 R08: 0000000000000000 R09: ffffac29c07dbbb8
[  +0.000002] R10: 0000000000000001 R11: 0000000000000001 R12: ffffffff8b20f5ff
[  +0.000001] R13: 0000000000000000 R14: ffffffff8bc72c10 R15: ffffa3cbc18c56c8
[  +0.000002] FS:  00007fc5146f14c0(0000) GS:ffffa3cc78300000(0000) knlGS:0000000000000000
[  +0.000002] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  +0.000001] CR2: 0000000000402865 CR3: 0000000025fe0002 CR4: 00000000000306e0
[  +0.000003] Call Trace:
[  +0.000009]  devinet_sysctl_forward+0x1b1/0x1e0
[  +0.000006]  proc_sys_call_handler+0x12e/0x230
[  +0.000005]  new_sync_write+0x11f/0x1b0
[  +0.000004]  vfs_write+0x1c0/0x280
[  +0.000103]  ksys_write+0x5f/0xe0
[  +0.000351]  do_syscall_64+0x33/0x40
[  +0.000004]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
[  +0.000003] RIP: 0033:0x7fc514cb9a93
[  +0.000004] Code: 0d 00 f7 d8 64 89 02 48 c7 c0 ff ff ff ff eb bb 0f 1f 80 00 00 00 00 64 8b 04 25 18 00 00 00 85 c0 75 14 b8 01 00 00 00 0f 05 <48> 3d 00 f0 ff ff 77 55 c3 0f 1f 40 00 48 83 ec 28 48 89 54 24 18
[  +0.000001] RSP: 002b:00007ffc05e1ddf8 EFLAGS: 00000246 ORIG_RAX: 0000000000000001
[  +0.000003] RAX: ffffffffffffffda RBX: 00007ffc05e1de00 RCX: 00007fc514cb9a93
[  +0.000001] RDX: 0000000000000002 RSI: 00007ffc05e1de00 RDI: 0000000000000004
[  +0.000002] RBP: 00007ffc05e1de70 R08: 000000000000c1c2 R09: 77726f665f70692f
[  +0.000001] R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000
[  +0.000001] R13: 0000000000000004 R14: 0000000000000002 R15: 00007ffc05e1dee0
[  +0.000003] ---[ end trace 688883f998caf291 ]---
[  +6.415616] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000027] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000004] NFSD: Unable to initialize client recovery tracking! (-2)
[ +14.132186] systemd-fstab-generator[650]: Ignoring "noauto" for root device
[  +0.427129] systemd-fstab-generator[661]: Ignoring "noauto" for root device
[Dec24 08:07] systemd-fstab-generator[911]: Ignoring "noauto" for root device
[  +0.584231] systemd-fstab-generator[922]: Ignoring "noauto" for root device
[  +0.601625] systemd-fstab-generator[933]: Ignoring "noauto" for root device
[  +3.534530] kauditd_printk_skb: 28 callbacks suppressed
[  +1.718990] systemd-fstab-generator[1094]: Ignoring "noauto" for root device
[  +0.653174] systemd-fstab-generator[1105]: Ignoring "noauto" for root device
[ +33.603990] systemd-fstab-generator[1342]: Ignoring "noauto" for root device
[Dec24 08:08] kauditd_printk_skb: 29 callbacks suppressed
[Dec24 08:09] kauditd_printk_skb: 8 callbacks suppressed
[Dec24 08:11] kauditd_printk_skb: 3 callbacks suppressed
[Dec24 08:15] hrtimer: interrupt took 5140314 ns

* 
* ==> etcd [6035996a660d] <==
* {"level":"info","ts":"2022-12-24T06:46:24.799Z","caller":"traceutil/trace.go:171","msg":"trace[2133263865] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:29900; }","duration":"695.794277ms","start":"2022-12-24T06:46:24.103Z","end":"2022-12-24T06:46:24.799Z","steps":["trace[2133263865] 'agreement among raft nodes before linearized reading'  (duration: 695.722469ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T06:46:24.799Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T06:46:24.103Z","time spent":"695.847364ms","remote":"127.0.0.1:35420","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1136,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2022-12-24T06:47:16.167Z","caller":"traceutil/trace.go:171","msg":"trace[572332918] transaction","detail":"{read_only:false; response_revision:29947; number_of_response:1; }","duration":"140.912016ms","start":"2022-12-24T06:47:16.026Z","end":"2022-12-24T06:47:16.167Z","steps":["trace[572332918] 'process raft request'  (duration: 50.212112ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T06:47:49.257Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"171.122135ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:47:49.267Z","caller":"traceutil/trace.go:171","msg":"trace[1911268217] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:29968; }","duration":"180.303857ms","start":"2022-12-24T06:47:49.086Z","end":"2022-12-24T06:47:49.267Z","steps":["trace[1911268217] 'agreement among raft nodes before linearized reading'  (duration: 63.60914ms)","trace[1911268217] 'range keys from in-memory index tree'  (duration: 107.47941ms)"],"step_count":2}
{"level":"info","ts":"2022-12-24T06:48:06.388Z","caller":"traceutil/trace.go:171","msg":"trace[1374594304] linearizableReadLoop","detail":"{readStateIndex:39067; appliedIndex:39067; }","duration":"102.751992ms","start":"2022-12-24T06:48:06.286Z","end":"2022-12-24T06:48:06.388Z","steps":["trace[1374594304] 'read index received'  (duration: 102.663393ms)","trace[1374594304] 'applied index is now lower than readState.Index'  (duration: 85.129µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:48:06.532Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"246.735765ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:48:06.543Z","caller":"traceutil/trace.go:171","msg":"trace[1405627743] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:29978; }","duration":"257.043403ms","start":"2022-12-24T06:48:06.286Z","end":"2022-12-24T06:48:06.543Z","steps":["trace[1405627743] 'agreement among raft nodes before linearized reading'  (duration: 170.875569ms)","trace[1405627743] 'range keys from in-memory index tree'  (duration: 75.81742ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:48:31.869Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.03358915s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:48:31.870Z","caller":"traceutil/trace.go:171","msg":"trace[1959222671] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:29993; }","duration":"1.034677239s","start":"2022-12-24T06:48:30.836Z","end":"2022-12-24T06:48:31.870Z","steps":["trace[1959222671] 'count revisions from in-memory index tree'  (duration: 1.03254172s)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T06:48:31.870Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T06:48:30.836Z","time spent":"1.034786732s","remote":"127.0.0.1:35438","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":0,"response size":30,"request content":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true "}
{"level":"warn","ts":"2022-12-24T06:48:31.905Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.912789ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/\" range_end:\"/registry/rolebindings0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-12-24T06:48:31.905Z","caller":"traceutil/trace.go:171","msg":"trace[2085720940] range","detail":"{range_begin:/registry/rolebindings/; range_end:/registry/rolebindings0; response_count:0; response_revision:29993; }","duration":"105.039546ms","start":"2022-12-24T06:48:31.800Z","end":"2022-12-24T06:48:31.905Z","steps":["trace[2085720940] 'count revisions from in-memory index tree'  (duration: 98.792735ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T06:49:29.471Z","caller":"traceutil/trace.go:171","msg":"trace[1465262739] transaction","detail":"{read_only:false; response_revision:30030; number_of_response:1; }","duration":"114.242236ms","start":"2022-12-24T06:49:29.347Z","end":"2022-12-24T06:49:29.461Z","steps":["trace[1465262739] 'process raft request'  (duration: 94.373896ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T06:49:59.277Z","caller":"traceutil/trace.go:171","msg":"trace[1090047623] linearizableReadLoop","detail":"{readStateIndex:39158; appliedIndex:39158; }","duration":"125.214159ms","start":"2022-12-24T06:49:59.152Z","end":"2022-12-24T06:49:59.277Z","steps":["trace[1090047623] 'read index received'  (duration: 124.676905ms)","trace[1090047623] 'applied index is now lower than readState.Index'  (duration: 533.811µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:49:59.297Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"137.097938ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/\" range_end:\"/registry/minions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-12-24T06:49:59.297Z","caller":"traceutil/trace.go:171","msg":"trace[1043745526] range","detail":"{range_begin:/registry/minions/; range_end:/registry/minions0; response_count:0; response_revision:30047; }","duration":"145.119856ms","start":"2022-12-24T06:49:59.152Z","end":"2022-12-24T06:49:59.297Z","steps":["trace[1043745526] 'agreement among raft nodes before linearized reading'  (duration: 137.044341ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T06:49:59.738Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29849}
{"level":"info","ts":"2022-12-24T06:49:59.739Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":29849,"took":"778.755µs"}
{"level":"warn","ts":"2022-12-24T06:50:24.261Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"107.891675ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:50:24.265Z","caller":"traceutil/trace.go:171","msg":"trace[322293536] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:30063; }","duration":"111.134418ms","start":"2022-12-24T06:50:24.153Z","end":"2022-12-24T06:50:24.264Z","steps":["trace[322293536] 'agreement among raft nodes before linearized reading'  (duration: 107.842927ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T06:50:35.977Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.827676ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.59.102\" ","response":"range_response_count:1 size:139"}
{"level":"info","ts":"2022-12-24T06:50:35.979Z","caller":"traceutil/trace.go:171","msg":"trace[1703121854] range","detail":"{range_begin:/registry/masterleases/192.168.59.102; range_end:; response_count:1; response_revision:30070; }","duration":"105.535682ms","start":"2022-12-24T06:50:35.873Z","end":"2022-12-24T06:50:35.979Z","steps":["trace[1703121854] 'agreement among raft nodes before linearized reading'  (duration: 50.048157ms)","trace[1703121854] 'range keys from in-memory index tree'  (duration: 53.722969ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:50:36.443Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"178.098903ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:423"}
{"level":"info","ts":"2022-12-24T06:50:36.496Z","caller":"traceutil/trace.go:171","msg":"trace[1885558072] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:30071; }","duration":"231.110572ms","start":"2022-12-24T06:50:36.265Z","end":"2022-12-24T06:50:36.496Z","steps":["trace[1885558072] 'range keys from in-memory index tree'  (duration: 177.396794ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T06:50:55.957Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"799.613631ms","expected-duration":"100ms","prefix":"","request":"header:<ID:16341739244559321634 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.59.102\" mod_revision:30076 > success:<request_put:<key:\"/registry/masterleases/192.168.59.102\" value_size:67 lease:7118367207704545824 >> failure:<request_range:<key:\"/registry/masterleases/192.168.59.102\" > >>","response":"size:18"}
{"level":"info","ts":"2022-12-24T06:50:55.978Z","caller":"traceutil/trace.go:171","msg":"trace[593684762] transaction","detail":"{read_only:false; response_revision:30082; number_of_response:1; }","duration":"832.970134ms","start":"2022-12-24T06:50:55.145Z","end":"2022-12-24T06:50:55.978Z","steps":["trace[593684762] 'process raft request'  (duration: 11.677014ms)","trace[593684762] 'compare'  (duration: 687.681207ms)","trace[593684762] 'store kv pair into bolt db' {req_type:put; key:/registry/masterleases/192.168.59.102; req_size:118; } (duration: 111.829443ms)"],"step_count":3}
{"level":"info","ts":"2022-12-24T06:50:55.998Z","caller":"traceutil/trace.go:171","msg":"trace[1693938394] linearizableReadLoop","detail":"{readStateIndex:39207; appliedIndex:39205; }","duration":"779.234023ms","start":"2022-12-24T06:50:55.178Z","end":"2022-12-24T06:50:55.957Z","steps":["trace[1693938394] 'read index received'  (duration: 105.386µs)","trace[1693938394] 'applied index is now lower than readState.Index'  (duration: 779.127688ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:50:56.045Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T06:50:55.145Z","time spent":"833.575114ms","remote":"127.0.0.1:35400","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":120,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.59.102\" mod_revision:30076 > success:<request_put:<key:\"/registry/masterleases/192.168.59.102\" value_size:67 lease:7118367207704545824 >> failure:<request_range:<key:\"/registry/masterleases/192.168.59.102\" > >"}
{"level":"info","ts":"2022-12-24T06:50:56.051Z","caller":"traceutil/trace.go:171","msg":"trace[883042748] transaction","detail":"{read_only:false; response_revision:30083; number_of_response:1; }","duration":"854.934216ms","start":"2022-12-24T06:50:55.157Z","end":"2022-12-24T06:50:56.012Z","steps":["trace[883042748] 'process raft request'  (duration: 800.132817ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T06:50:56.052Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T06:50:55.157Z","time spent":"894.798589ms","remote":"127.0.0.1:35420","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:30081 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2022-12-24T06:50:56.145Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"267.604594ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-12-24T06:50:56.145Z","caller":"traceutil/trace.go:171","msg":"trace[1361935321] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:30083; }","duration":"268.3857ms","start":"2022-12-24T06:50:55.877Z","end":"2022-12-24T06:50:56.145Z","steps":["trace[1361935321] 'agreement among raft nodes before linearized reading'  (duration: 167.359592ms)","trace[1361935321] 'count revisions from in-memory index tree'  (duration: 100.206683ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:50:56.157Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"979.449273ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:50:56.157Z","caller":"traceutil/trace.go:171","msg":"trace[1389193126] range","detail":"{range_begin:/registry/csistoragecapacities/; range_end:/registry/csistoragecapacities0; response_count:0; response_revision:30083; }","duration":"979.607069ms","start":"2022-12-24T06:50:55.178Z","end":"2022-12-24T06:50:56.157Z","steps":["trace[1389193126] 'agreement among raft nodes before linearized reading'  (duration: 872.928954ms)","trace[1389193126] 'count revisions from in-memory index tree'  (duration: 106.473331ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:50:56.160Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T06:50:55.178Z","time spent":"982.42958ms","remote":"127.0.0.1:35482","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":0,"response size":30,"request content":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true "}
{"level":"warn","ts":"2022-12-24T06:50:56.185Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"329.442359ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:50:56.205Z","caller":"traceutil/trace.go:171","msg":"trace[255550309] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:30083; }","duration":"350.023373ms","start":"2022-12-24T06:50:55.855Z","end":"2022-12-24T06:50:56.205Z","steps":["trace[255550309] 'agreement among raft nodes before linearized reading'  (duration: 173.110341ms)","trace[255550309] 'range keys from in-memory index tree'  (duration: 156.010323ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:50:56.206Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T06:50:55.855Z","time spent":"350.201012ms","remote":"127.0.0.1:35378","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-12-24T06:51:55.203Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"113.848156ms","expected-duration":"100ms","prefix":"","request":"header:<ID:16341739244559321936 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.59.102\" mod_revision:30123 > success:<request_put:<key:\"/registry/masterleases/192.168.59.102\" value_size:67 lease:7118367207704546125 >> failure:<request_range:<key:\"/registry/masterleases/192.168.59.102\" > >>","response":"size:18"}
{"level":"info","ts":"2022-12-24T06:51:55.204Z","caller":"traceutil/trace.go:171","msg":"trace[1201317462] linearizableReadLoop","detail":"{readStateIndex:39266; appliedIndex:39265; }","duration":"107.130706ms","start":"2022-12-24T06:51:55.097Z","end":"2022-12-24T06:51:55.204Z","steps":["trace[1201317462] 'read index received'  (duration: 2.923645ms)","trace[1201317462] 'applied index is now lower than readState.Index'  (duration: 104.205333ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:51:55.208Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"111.036766ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:51:55.209Z","caller":"traceutil/trace.go:171","msg":"trace[83782998] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:30129; }","duration":"111.917431ms","start":"2022-12-24T06:51:55.097Z","end":"2022-12-24T06:51:55.209Z","steps":["trace[83782998] 'agreement among raft nodes before linearized reading'  (duration: 110.896352ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T06:51:55.214Z","caller":"traceutil/trace.go:171","msg":"trace[2056927431] transaction","detail":"{read_only:false; response_revision:30129; number_of_response:1; }","duration":"151.267008ms","start":"2022-12-24T06:51:55.062Z","end":"2022-12-24T06:51:55.214Z","steps":["trace[2056927431] 'process raft request'  (duration: 24.699422ms)","trace[2056927431] 'compare'  (duration: 104.003494ms)"],"step_count":2}
{"level":"info","ts":"2022-12-24T06:52:00.901Z","caller":"traceutil/trace.go:171","msg":"trace[419740761] transaction","detail":"{read_only:false; response_revision:30138; number_of_response:1; }","duration":"914.530235ms","start":"2022-12-24T06:51:59.987Z","end":"2022-12-24T06:52:00.901Z","steps":["trace[419740761] 'process raft request'  (duration: 909.315696ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T06:52:00.901Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T06:51:59.987Z","time spent":"914.691584ms","remote":"127.0.0.1:35420","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:30131 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2022-12-24T06:52:09.864Z","caller":"traceutil/trace.go:171","msg":"trace[859403768] linearizableReadLoop","detail":"{readStateIndex:39282; appliedIndex:39282; }","duration":"126.145454ms","start":"2022-12-24T06:52:09.738Z","end":"2022-12-24T06:52:09.864Z","steps":["trace[859403768] 'read index received'  (duration: 126.134196ms)","trace[859403768] 'applied index is now lower than readState.Index'  (duration: 9.452µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:52:09.870Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"131.196575ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:52:09.871Z","caller":"traceutil/trace.go:171","msg":"trace[1100822284] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:30143; }","duration":"133.007319ms","start":"2022-12-24T06:52:09.738Z","end":"2022-12-24T06:52:09.871Z","steps":["trace[1100822284] 'agreement among raft nodes before linearized reading'  (duration: 130.263869ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T06:52:25.328Z","caller":"traceutil/trace.go:171","msg":"trace[824560956] transaction","detail":"{read_only:false; response_revision:30153; number_of_response:1; }","duration":"223.138085ms","start":"2022-12-24T06:52:25.104Z","end":"2022-12-24T06:52:25.328Z","steps":["trace[824560956] 'process raft request'  (duration: 217.208411ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T06:52:30.899Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"479.763067ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:52:30.900Z","caller":"traceutil/trace.go:171","msg":"trace[467165367] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:30156; }","duration":"479.955303ms","start":"2022-12-24T06:52:30.420Z","end":"2022-12-24T06:52:30.899Z","steps":["trace[467165367] 'agreement among raft nodes before linearized reading'  (duration: 450.3665ms)","trace[467165367] 'range keys from in-memory index tree'  (duration: 29.36805ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T06:52:30.901Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T06:52:30.420Z","time spent":"480.965395ms","remote":"127.0.0.1:35378","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-12-24T06:54:16.345Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.149779ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T06:54:16.403Z","caller":"traceutil/trace.go:171","msg":"trace[13970586] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:30230; }","duration":"163.856631ms","start":"2022-12-24T06:54:16.239Z","end":"2022-12-24T06:54:16.402Z","steps":["trace[13970586] 'range keys from in-memory index tree'  (duration: 105.847308ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T06:54:56.427Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"118.192489ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:139"}
{"level":"info","ts":"2022-12-24T06:54:56.444Z","caller":"traceutil/trace.go:171","msg":"trace[868863994] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:30256; }","duration":"135.804206ms","start":"2022-12-24T06:54:56.309Z","end":"2022-12-24T06:54:56.444Z","steps":["trace[868863994] 'range keys from in-memory index tree'  (duration: 118.016759ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T06:55:00.174Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30048}
{"level":"info","ts":"2022-12-24T06:55:00.175Z","caller":"traceutil/trace.go:171","msg":"trace[947419849] compact","detail":"{revision:30048; response_revision:30259; }","duration":"112.382283ms","start":"2022-12-24T06:55:00.062Z","end":"2022-12-24T06:55:00.175Z","steps":["trace[947419849] 'check and update compact revision'  (duration: 99.306498ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T06:55:00.208Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":30048,"took":"1.317266ms"}

* 
* ==> etcd [ee0fe6950d4a] <==
* {"level":"info","ts":"2022-12-24T11:17:23.082Z","caller":"traceutil/trace.go:171","msg":"trace[1359434206] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:37917; }","duration":"142.440753ms","start":"2022-12-24T11:17:22.940Z","end":"2022-12-24T11:17:23.082Z","steps":["trace[1359434206] 'range keys from in-memory index tree'  (duration: 141.778007ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T11:17:23.483Z","caller":"traceutil/trace.go:171","msg":"trace[2100993439] linearizableReadLoop","detail":"{readStateIndex:49460; appliedIndex:49460; }","duration":"177.686856ms","start":"2022-12-24T11:17:23.305Z","end":"2022-12-24T11:17:23.482Z","steps":["trace[2100993439] 'read index received'  (duration: 177.672139ms)","trace[2100993439] 'applied index is now lower than readState.Index'  (duration: 12.633µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:17:23.486Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"183.243614ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:17:23.563Z","caller":"traceutil/trace.go:171","msg":"trace[128717732] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:37917; }","duration":"260.135908ms","start":"2022-12-24T11:17:23.303Z","end":"2022-12-24T11:17:23.563Z","steps":["trace[128717732] 'agreement among raft nodes before linearized reading'  (duration: 183.186075ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T11:17:23.564Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T11:17:23.259Z","time spent":"303.764995ms","remote":"127.0.0.1:46934","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-12-24T11:17:24.645Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"269.098039ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:17:24.657Z","caller":"traceutil/trace.go:171","msg":"trace[25409168] transaction","detail":"{read_only:false; response_revision:37919; number_of_response:1; }","duration":"122.287764ms","start":"2022-12-24T11:17:24.534Z","end":"2022-12-24T11:17:24.657Z","steps":["trace[25409168] 'process raft request'  (duration: 30.965526ms)","trace[25409168] 'compare'  (duration: 76.535287ms)","trace[25409168] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 13.954279ms)"],"step_count":3}
{"level":"info","ts":"2022-12-24T11:17:24.774Z","caller":"traceutil/trace.go:171","msg":"trace[1733388345] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:37918; }","duration":"397.994116ms","start":"2022-12-24T11:17:24.376Z","end":"2022-12-24T11:17:24.774Z","steps":["trace[1733388345] 'agreement among raft nodes before linearized reading'  (duration: 55.558352ms)","trace[1733388345] 'range keys from in-memory index tree'  (duration: 213.504464ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:17:24.775Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T11:17:24.376Z","time spent":"399.01675ms","remote":"127.0.0.1:46934","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-12-24T11:18:02.894Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"232.47014ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:18:02.894Z","caller":"traceutil/trace.go:171","msg":"trace[662710353] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:37941; }","duration":"232.625414ms","start":"2022-12-24T11:18:02.661Z","end":"2022-12-24T11:18:02.894Z","steps":["trace[662710353] 'range keys from in-memory index tree'  (duration: 232.446348ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T11:18:02.898Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"237.286624ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" ","response":"range_response_count:1 size:482"}
{"level":"info","ts":"2022-12-24T11:18:02.898Z","caller":"traceutil/trace.go:171","msg":"trace[317150377] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:37941; }","duration":"237.408749ms","start":"2022-12-24T11:18:02.660Z","end":"2022-12-24T11:18:02.898Z","steps":["trace[317150377] 'range keys from in-memory index tree'  (duration: 236.832342ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T11:19:03.923Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":37791}
{"level":"info","ts":"2022-12-24T11:19:03.934Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":37791,"took":"2.586015ms"}
{"level":"warn","ts":"2022-12-24T11:19:12.201Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"100.033041ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:19:12.251Z","caller":"traceutil/trace.go:171","msg":"trace[1603695982] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:37985; }","duration":"150.840525ms","start":"2022-12-24T11:19:12.101Z","end":"2022-12-24T11:19:12.251Z","steps":["trace[1603695982] 'agreement among raft nodes before linearized reading'  (duration: 99.998095ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T11:19:12.498Z","caller":"traceutil/trace.go:171","msg":"trace[922152759] linearizableReadLoop","detail":"{readStateIndex:49552; appliedIndex:49552; }","duration":"105.345059ms","start":"2022-12-24T11:19:12.393Z","end":"2022-12-24T11:19:12.498Z","steps":["trace[922152759] 'read index received'  (duration: 105.331119ms)","trace[922152759] 'applied index is now lower than readState.Index'  (duration: 12.407µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:19:12.504Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"111.529924ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:19:12.505Z","caller":"traceutil/trace.go:171","msg":"trace[1587702624] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:37986; }","duration":"111.916809ms","start":"2022-12-24T11:19:12.393Z","end":"2022-12-24T11:19:12.505Z","steps":["trace[1587702624] 'agreement among raft nodes before linearized reading'  (duration: 105.860178ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T11:19:12.534Z","caller":"traceutil/trace.go:171","msg":"trace[1591595760] transaction","detail":"{read_only:false; response_revision:37987; number_of_response:1; }","duration":"138.688166ms","start":"2022-12-24T11:19:12.396Z","end":"2022-12-24T11:19:12.534Z","steps":["trace[1591595760] 'process raft request'  (duration: 102.846499ms)","trace[1591595760] 'compare'  (duration: 35.341627ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:19:39.669Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":16341739245513058918,"retry-timeout":"500ms"}
{"level":"info","ts":"2022-12-24T11:19:39.818Z","caller":"traceutil/trace.go:171","msg":"trace[1267231282] linearizableReadLoop","detail":"{readStateIndex:49575; appliedIndex:49575; }","duration":"714.276842ms","start":"2022-12-24T11:19:39.104Z","end":"2022-12-24T11:19:39.818Z","steps":["trace[1267231282] 'read index received'  (duration: 714.255872ms)","trace[1267231282] 'applied index is now lower than readState.Index'  (duration: 18.919µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:19:39.876Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"772.273683ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:19:39.878Z","caller":"traceutil/trace.go:171","msg":"trace[269325669] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:38004; }","duration":"773.438739ms","start":"2022-12-24T11:19:39.104Z","end":"2022-12-24T11:19:39.878Z","steps":["trace[269325669] 'agreement among raft nodes before linearized reading'  (duration: 715.859433ms)","trace[269325669] 'range keys from in-memory index tree'  (duration: 56.402999ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:19:39.878Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-12-24T11:19:39.104Z","time spent":"773.593355ms","remote":"127.0.0.1:46934","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-12-24T11:20:08.087Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.818226ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:20:08.089Z","caller":"traceutil/trace.go:171","msg":"trace[1371945385] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:38022; }","duration":"110.076569ms","start":"2022-12-24T11:20:07.979Z","end":"2022-12-24T11:20:08.089Z","steps":["trace[1371945385] 'agreement among raft nodes before linearized reading'  (duration: 82.130403ms)","trace[1371945385] 'range keys from in-memory index tree'  (duration: 24.365722ms)"],"step_count":2}
{"level":"info","ts":"2022-12-24T11:20:11.858Z","caller":"traceutil/trace.go:171","msg":"trace[733169622] linearizableReadLoop","detail":"{readStateIndex:49601; appliedIndex:49601; }","duration":"136.456526ms","start":"2022-12-24T11:20:11.722Z","end":"2022-12-24T11:20:11.858Z","steps":["trace[733169622] 'read index received'  (duration: 136.441037ms)","trace[733169622] 'applied index is now lower than readState.Index'  (duration: 13.316µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:20:11.859Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"137.200061ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csidrivers/\" range_end:\"/registry/csidrivers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:20:11.859Z","caller":"traceutil/trace.go:171","msg":"trace[719726715] range","detail":"{range_begin:/registry/csidrivers/; range_end:/registry/csidrivers0; response_count:0; response_revision:38024; }","duration":"137.27659ms","start":"2022-12-24T11:20:11.722Z","end":"2022-12-24T11:20:11.859Z","steps":["trace[719726715] 'agreement among raft nodes before linearized reading'  (duration: 137.1738ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T11:21:12.064Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"147.091957ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:342"}
{"level":"info","ts":"2022-12-24T11:21:12.065Z","caller":"traceutil/trace.go:171","msg":"trace[226382505] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:38064; }","duration":"163.148845ms","start":"2022-12-24T11:21:11.902Z","end":"2022-12-24T11:21:12.065Z","steps":["trace[226382505] 'range keys from in-memory index tree'  (duration: 146.755635ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T11:21:12.590Z","caller":"traceutil/trace.go:171","msg":"trace[422509339] transaction","detail":"{read_only:false; response_revision:38066; number_of_response:1; }","duration":"129.67521ms","start":"2022-12-24T11:21:12.461Z","end":"2022-12-24T11:21:12.590Z","steps":["trace[422509339] 'process raft request'  (duration: 48.885723ms)","trace[422509339] 'compare'  (duration: 56.885446ms)","trace[422509339] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/masterleases/192.168.59.102; req_size:119; } (duration: 19.221565ms)"],"step_count":3}
{"level":"info","ts":"2022-12-24T11:21:22.126Z","caller":"traceutil/trace.go:171","msg":"trace[1408202349] linearizableReadLoop","detail":"{readStateIndex:49665; appliedIndex:49665; }","duration":"133.037587ms","start":"2022-12-24T11:21:21.993Z","end":"2022-12-24T11:21:22.126Z","steps":["trace[1408202349] 'read index received'  (duration: 133.023336ms)","trace[1408202349] 'applied index is now lower than readState.Index'  (duration: 12.367µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:21:22.171Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"177.434544ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:21:22.173Z","caller":"traceutil/trace.go:171","msg":"trace[1585229769] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:38072; }","duration":"180.112594ms","start":"2022-12-24T11:21:21.993Z","end":"2022-12-24T11:21:22.173Z","steps":["trace[1585229769] 'agreement among raft nodes before linearized reading'  (duration: 133.470681ms)","trace[1585229769] 'range keys from in-memory index tree'  (duration: 43.362028ms)"],"step_count":2}
{"level":"info","ts":"2022-12-24T11:21:22.178Z","caller":"traceutil/trace.go:171","msg":"trace[1630452408] transaction","detail":"{read_only:false; response_revision:38073; number_of_response:1; }","duration":"147.810391ms","start":"2022-12-24T11:21:22.030Z","end":"2022-12-24T11:21:22.178Z","steps":["trace[1630452408] 'process raft request'  (duration: 96.857431ms)","trace[1630452408] 'compare'  (duration: 46.294767ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:22:22.109Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"130.799412ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:342"}
{"level":"info","ts":"2022-12-24T11:22:22.110Z","caller":"traceutil/trace.go:171","msg":"trace[1085677245] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:38114; }","duration":"131.983939ms","start":"2022-12-24T11:22:21.978Z","end":"2022-12-24T11:22:22.110Z","steps":["trace[1085677245] 'agreement among raft nodes before linearized reading'  (duration: 130.71766ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T11:22:36.770Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"117.151022ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:22:36.777Z","caller":"traceutil/trace.go:171","msg":"trace[862653108] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:38125; }","duration":"123.626782ms","start":"2022-12-24T11:22:36.653Z","end":"2022-12-24T11:22:36.777Z","steps":["trace[862653108] 'count revisions from in-memory index tree'  (duration: 115.508127ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T11:24:04.155Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"134.875632ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:24:04.155Z","caller":"traceutil/trace.go:171","msg":"trace[1288626922] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:38181; }","duration":"135.222551ms","start":"2022-12-24T11:24:04.020Z","end":"2022-12-24T11:24:04.155Z","steps":["trace[1288626922] 'range keys from in-memory index tree'  (duration: 134.637753ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T11:24:04.155Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"135.913996ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:24:04.155Z","caller":"traceutil/trace.go:171","msg":"trace[502420917] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:38181; }","duration":"136.009877ms","start":"2022-12-24T11:24:04.019Z","end":"2022-12-24T11:24:04.155Z","steps":["trace[502420917] 'count revisions from in-memory index tree'  (duration: 135.348039ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T11:24:04.156Z","caller":"traceutil/trace.go:171","msg":"trace[1895683616] transaction","detail":"{read_only:false; response_revision:38182; number_of_response:1; }","duration":"135.182688ms","start":"2022-12-24T11:24:04.021Z","end":"2022-12-24T11:24:04.156Z","steps":["trace[1895683616] 'process raft request'  (duration: 58.706428ms)","trace[1895683616] 'compare'  (duration: 76.360032ms)"],"step_count":2}
{"level":"info","ts":"2022-12-24T11:24:04.263Z","caller":"traceutil/trace.go:171","msg":"trace[111498681] compact","detail":"{revision:37981; response_revision:38182; }","duration":"103.613917ms","start":"2022-12-24T11:24:04.159Z","end":"2022-12-24T11:24:04.263Z","steps":["trace[111498681] 'check and update compact revision'  (duration: 79.22752ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T11:24:04.264Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":37981}
{"level":"info","ts":"2022-12-24T11:24:04.315Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":37981,"took":"50.689796ms"}
{"level":"warn","ts":"2022-12-24T11:24:20.334Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"145.530958ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper\" ","response":"range_response_count:1 size:888"}
{"level":"info","ts":"2022-12-24T11:24:20.334Z","caller":"traceutil/trace.go:171","msg":"trace[1491158798] range","detail":"{range_begin:/registry/services/endpoints/kubernetes-dashboard/dashboard-metrics-scraper; range_end:; response_count:1; response_revision:38191; }","duration":"145.698109ms","start":"2022-12-24T11:24:20.188Z","end":"2022-12-24T11:24:20.334Z","steps":["trace[1491158798] 'agreement among raft nodes before linearized reading'  (duration: 145.403833ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T11:24:20.357Z","caller":"traceutil/trace.go:171","msg":"trace[1439653466] linearizableReadLoop","detail":"{readStateIndex:49821; appliedIndex:49821; }","duration":"168.715102ms","start":"2022-12-24T11:24:20.188Z","end":"2022-12-24T11:24:20.357Z","steps":["trace[1439653466] 'read index received'  (duration: 106.753311ms)","trace[1439653466] 'applied index is now lower than readState.Index'  (duration: 61.959176ms)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:26:01.479Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.170701ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-12-24T11:26:01.479Z","caller":"traceutil/trace.go:171","msg":"trace[991139825] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:38258; }","duration":"108.434374ms","start":"2022-12-24T11:26:01.370Z","end":"2022-12-24T11:26:01.479Z","steps":["trace[991139825] 'range keys from in-memory index tree'  (duration: 103.802509ms)"],"step_count":1}
{"level":"warn","ts":"2022-12-24T11:26:03.896Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"119.477431ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/roles/\" range_end:\"/registry/roles0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-12-24T11:26:03.900Z","caller":"traceutil/trace.go:171","msg":"trace[1477284801] range","detail":"{range_begin:/registry/roles/; range_end:/registry/roles0; response_count:0; response_revision:38260; }","duration":"123.214539ms","start":"2022-12-24T11:26:03.777Z","end":"2022-12-24T11:26:03.900Z","steps":["trace[1477284801] 'count revisions from in-memory index tree'  (duration: 119.156072ms)"],"step_count":1}
{"level":"info","ts":"2022-12-24T11:26:32.986Z","caller":"traceutil/trace.go:171","msg":"trace[1110019928] linearizableReadLoop","detail":"{readStateIndex:49937; appliedIndex:49937; }","duration":"115.384755ms","start":"2022-12-24T11:26:32.871Z","end":"2022-12-24T11:26:32.986Z","steps":["trace[1110019928] 'read index received'  (duration: 115.373351ms)","trace[1110019928] 'applied index is now lower than readState.Index'  (duration: 9.169µs)"],"step_count":2}
{"level":"warn","ts":"2022-12-24T11:26:32.996Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"125.860916ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:423"}
{"level":"info","ts":"2022-12-24T11:26:32.997Z","caller":"traceutil/trace.go:171","msg":"trace[1352575751] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:38279; }","duration":"125.966821ms","start":"2022-12-24T11:26:32.871Z","end":"2022-12-24T11:26:32.997Z","steps":["trace[1352575751] 'agreement among raft nodes before linearized reading'  (duration: 125.095009ms)"],"step_count":1}

* 
* ==> kernel <==
*  11:27:23 up  3:21,  0 users,  load average: 9.93, 12.09, 12.17
Linux minikube 5.10.57 #1 SMP Fri Oct 28 21:02:11 UTC 2022 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kube-apiserver [6346360436cc] <==
* I1224 06:50:56.110095       1 trace.go:205] Trace[695590536]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:b33a2780-8064-4bf2-b5e4-2d8abd986fcc,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 06:50:55.076) (total time: 1033ms):
Trace[695590536]: ---"Write to database call finished" len:1354,err:<nil> 1030ms (06:50:56.109)
Trace[695590536]: [1.033712813s] [1.033712813s] END
E1224 06:51:09.258116       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:51:09.258063490" prevR="56.81079725ss" incrR="184467440737.09550416ss" currentR="56.81078525ss"
I1224 06:51:37.869945       1 trace.go:205] Trace[2097920060]: "Create" url:/apis/authentication.k8s.io/v1/tokenreviews,user-agent:kubelet/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:fc2326e9-c91f-4943-8633-f3e50ffdd3ca,client:192.168.59.102,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (24-Dec-2022 06:51:37.118) (total time: 751ms):
Trace[2097920060]: ---"limitedReadBody done" len:1144,err:<nil> 750ms (06:51:37.869)
Trace[2097920060]: [751.120433ms] [751.120433ms] END
I1224 06:51:55.757726       1 trace.go:205] Trace[113380721]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:6b7b75c1-3f48-4742-a05f-4b5724e5f3c0,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 06:51:54.857) (total time: 900ms):
Trace[113380721]: ---"limitedReadBody done" len:1354,err:<nil> 849ms (06:51:55.707)
Trace[113380721]: [900.144464ms] [900.144464ms] END
I1224 06:52:01.155226       1 trace.go:205] Trace[364352613]: "GuaranteedUpdate etcd3" audit-id:467d6f23-7688-4fe2-a7ec-db1305af476a,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints (24-Dec-2022 06:51:59.984) (total time: 1170ms):
Trace[364352613]: ---"Txn call finished" err:<nil> 1168ms (06:52:01.155)
Trace[364352613]: [1.17065545s] [1.17065545s] END
I1224 06:52:01.155565       1 trace.go:205] Trace[434822495]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:467d6f23-7688-4fe2-a7ec-db1305af476a,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 06:51:59.916) (total time: 1239ms):
Trace[434822495]: ---"limitedReadBody done" len:1354,err:<nil> 67ms (06:51:59.983)
Trace[434822495]: ---"Write to database call finished" len:1354,err:<nil> 1170ms (06:52:01.155)
Trace[434822495]: [1.239483033s] [1.239483033s] END
I1224 06:52:05.611762       1 trace.go:205] Trace[1429465873]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.59.102,type:*v1.Endpoints (24-Dec-2022 06:52:05.101) (total time: 509ms):
Trace[1429465873]: ---"Txn call finished" err:<nil> 489ms (06:52:05.611)
Trace[1429465873]: [509.945214ms] [509.945214ms] END
E1224 06:52:10.227222       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:52:10.227144281" prevR="57.72386712ss" incrR="184467440737.09549088ss" currentR="57.72384184ss"
E1224 06:52:10.240249       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:52:10.240196583" prevR="57.73721340ss" incrR="184467440737.09550596ss" currentR="57.73720320ss"
E1224 06:52:10.242041       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:52:10.241975445" prevR="57.73911494ss" incrR="184467440737.09548295ss" currentR="57.73908173ss"
E1224 06:52:10.244743       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:52:10.244712092" prevR="57.74196771ss" incrR="184467440737.09547513ss" currentR="57.74192668ss"
E1224 06:52:10.244917       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:52:10.244888852" prevR="57.74224444ss" incrR="184467440737.09548108ss" currentR="57.74220936ss"
I1224 06:52:36.941891       1 trace.go:205] Trace[1087092183]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.59.102,type:*v1.Endpoints (24-Dec-2022 06:52:36.102) (total time: 839ms):
Trace[1087092183]: ---"initial value restored" 774ms (06:52:36.876)
Trace[1087092183]: ---"Transaction prepared" 60ms (06:52:36.936)
Trace[1087092183]: [839.787976ms] [839.787976ms] END
I1224 06:52:37.118992       1 trace.go:205] Trace[219093719]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:2790ef3a-dd02-4e04-8ee7-b5680f2c962c,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 06:52:36.129) (total time: 988ms):
Trace[219093719]: ---"limitedReadBody done" len:1354,err:<nil> 867ms (06:52:36.996)
Trace[219093719]: ---"Write to database call finished" len:1354,err:<nil> 119ms (06:52:37.117)
Trace[219093719]: [988.789427ms] [988.789427ms] END
I1224 06:52:40.961555       1 trace.go:205] Trace[516631100]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:44e6e2ac-b540-4fb0-aca3-1003a75111d2,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 06:52:39.944) (total time: 1017ms):
Trace[516631100]: ---"About to write a response" 1016ms (06:52:40.961)
Trace[516631100]: [1.017162325s] [1.017162325s] END
E1224 06:52:41.009614       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:52:41.009512214" prevR="57.97926626ss" incrR="184467440737.09549540ss" currentR="57.97924550ss"
E1224 06:52:41.009976       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:52:41.009892350" prevR="57.98002667ss" incrR="184467440737.09548166ss" currentR="57.97999217ss"
I1224 06:53:02.085176       1 trace.go:205] Trace[112378386]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:188ca856-601e-495d-b2cd-f5adc9a61ab0,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 06:53:01.351) (total time: 733ms):
Trace[112378386]: ---"limitedReadBody done" len:1354,err:<nil> 638ms (06:53:01.989)
Trace[112378386]: ---"Write to database call finished" len:1354,err:<nil> 90ms (06:53:02.084)
Trace[112378386]: [733.741319ms] [733.741319ms] END
E1224 06:53:11.277301       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:53:11.277182938" prevR="58.20326584ss" incrR="184467440737.09548505ss" currentR="58.20323473ss"
E1224 06:53:11.296588       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:53:11.296536406" prevR="58.23280680ss" incrR="184467440737.09550273ss" currentR="58.23279337ss"
E1224 06:53:11.313330       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:53:11.313281810" prevR="58.25591344ss" incrR="184467440737.09544720ss" currentR="58.25584448ss"
E1224 06:53:41.780538       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:53:41.780388872" prevR="58.92849576ss" incrR="184467440737.09543172ss" currentR="58.92841132ss"
I1224 06:53:56.979979       1 trace.go:205] Trace[1260886017]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.59.102,type:*v1.Endpoints (24-Dec-2022 06:53:56.043) (total time: 936ms):
Trace[1260886017]: ---"initial value restored" 74ms (06:53:56.117)
Trace[1260886017]: ---"Txn call finished" err:<nil> 856ms (06:53:56.979)
Trace[1260886017]: [936.111527ms] [936.111527ms] END
I1224 06:53:57.915062       1 trace.go:205] Trace[194181413]: "List(recursive=true) etcd3" audit-id:,key:/masterleases/,resourceVersion:0,resourceVersionMatch:NotOlderThan,limit:0,continue: (24-Dec-2022 06:53:57.056) (total time: 858ms):
Trace[194181413]: [858.732591ms] [858.732591ms] END
E1224 06:54:42.182039       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:54:42.182002422" prevR="59.39769626ss" incrR="184467440737.09547986ss" currentR="59.39765996ss"
E1224 06:54:42.207922       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:54:42.207830180" prevR="59.43635983ss" incrR="184467440737.09546320ss" currentR="59.43630687ss"
E1224 06:54:42.208944       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:54:42.208865194" prevR="59.43799053ss" incrR="184467440737.09551336ss" currentR="59.43798773ss"
E1224 06:54:42.246178       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:54:42.246113073" prevR="59.48765257ss" incrR="184467440737.09548599ss" currentR="59.48762240ss"
E1224 06:55:12.453063       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 06:55:12.453004077" prevR="59.70002769ss" incrR="184467440737.09545529ss" currentR="59.69996682ss"
I1224 06:55:46.994936       1 trace.go:205] Trace[2100235268]: "Get" url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:782e52ba-1876-46a0-896a-f84305344a4b,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (24-Dec-2022 06:55:46.126) (total time: 868ms):
Trace[2100235268]: ---"About to write a response" 868ms (06:55:46.994)
Trace[2100235268]: [868.458865ms] [868.458865ms] END

* 
* ==> kube-apiserver [676a16d46a2b] <==
* I1224 11:18:53.628003       1 trace.go:205] Trace[1939017263]: "GuaranteedUpdate etcd3" audit-id:f7af4efd-ba78-431e-9ced-029889c08f4e,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints (24-Dec-2022 11:18:52.977) (total time: 650ms):
Trace[1939017263]: ---"Txn call finished" err:<nil> 643ms (11:18:53.627)
Trace[1939017263]: [650.912326ms] [650.912326ms] END
I1224 11:18:53.628239       1 trace.go:205] Trace[1558866539]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:f7af4efd-ba78-431e-9ced-029889c08f4e,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 11:18:52.955) (total time: 672ms):
Trace[1558866539]: ---"Write to database call finished" len:1354,err:<nil> 654ms (11:18:53.628)
Trace[1558866539]: [672.753213ms] [672.753213ms] END
E1224 11:19:00.443751       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:19:00.443713133" prevR="188.44061572ss" incrR="184467440737.09551308ss" currentR="188.44061264ss"
E1224 11:19:00.446083       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:19:00.446052015" prevR="188.44320924ss" incrR="184467440737.09550007ss" currentR="188.44319315ss"
I1224 11:19:12.569615       1 trace.go:205] Trace[25107382]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.59.102,type:*v1.Endpoints (24-Dec-2022 11:19:12.001) (total time: 558ms):
Trace[25107382]: ---"initial value restored" 176ms (11:19:12.178)
Trace[25107382]: ---"Transaction prepared" 216ms (11:19:12.394)
Trace[25107382]: ---"Txn call finished" err:<nil> 165ms (11:19:12.560)
Trace[25107382]: [558.200145ms] [558.200145ms] END
E1224 11:19:30.523531       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:19:30.523448417" prevR="188.73061344ss" incrR="184467440737.09550732ss" currentR="188.73060460ss"
E1224 11:19:30.525862       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:19:30.525830407" prevR="188.73322183ss" incrR="184467440737.09549900ss" currentR="188.73320467ss"
E1224 11:19:30.526167       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:19:30.526110976" prevR="188.73355684ss" incrR="184467440737.09550782ss" currentR="188.73354850ss"
E1224 11:19:30.526334       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:19:30.526312509" prevR="188.73376282ss" incrR="184467440737.09550338ss" currentR="188.73375004ss"
E1224 11:20:02.002921       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:20:02.002885133" prevR="189.14958048ss" incrR="184467440737.09548236ss" currentR="189.14954668ss"
E1224 11:20:32.002706       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:20:32.002668018" prevR="189.37171647ss" incrR="184467440737.09549942ss" currentR="189.37169973ss"
E1224 11:21:02.353762       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:21:02.353721606" prevR="189.77992180ss" incrR="184467440737.09549247ss" currentR="189.77989811ss"
E1224 11:21:02.428666       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:21:02.428564794" prevR="189.85627657ss" incrR="184467440737.09550389ss" currentR="189.85626430ss"
E1224 11:21:32.582303       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:21:32.582265496" prevR="190.06815832ss" incrR="184467440737.09547835ss" currentR="190.06812051ss"
E1224 11:21:32.583447       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:21:32.583382083" prevR="190.06941768ss" incrR="184467440737.09550999ss" currentR="190.06941151ss"
E1224 11:21:32.621062       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:21:32.621029416" prevR="190.10778376ss" incrR="184467440737.09551005ss" currentR="190.10777765ss"
E1224 11:21:32.636762       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:21:32.636728613" prevR="190.12437509ss" incrR="184467440737.09550430ss" currentR="190.12436323ss"
E1224 11:21:32.647346       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:21:32.647244299" prevR="190.13457511ss" incrR="184467440737.09550516ss" currentR="190.13456411ss"
E1224 11:22:02.718489       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:22:02.718385906" prevR="190.18417949ss" incrR="184467440737.09550584ss" currentR="190.18416917ss"
E1224 11:22:02.745716       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:22:02.745640341" prevR="190.21798441ss" incrR="184467440737.09550183ss" currentR="190.21797008ss"
I1224 11:22:29.725403       1 trace.go:205] Trace[1266725648]: "GuaranteedUpdate etcd3" audit-id:80b675ca-3c35-4f21-8ab6-6aae34fb5656,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints (24-Dec-2022 11:22:28.979) (total time: 746ms):
Trace[1266725648]: ---"Txn call finished" err:<nil> 744ms (11:22:29.725)
Trace[1266725648]: [746.005705ms] [746.005705ms] END
I1224 11:22:29.725719       1 trace.go:205] Trace[241122117]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:80b675ca-3c35-4f21-8ab6-6aae34fb5656,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 11:22:28.978) (total time: 747ms):
Trace[241122117]: ---"Write to database call finished" len:1354,err:<nil> 746ms (11:22:29.725)
Trace[241122117]: [747.049358ms] [747.049358ms] END
E1224 11:22:32.799652       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:22:32.799567150" prevR="190.36758043ss" incrR="184467440737.09550944ss" currentR="190.36757371ss"
E1224 11:22:32.806258       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:22:32.806222844" prevR="190.37453807ss" incrR="184467440737.09550520ss" currentR="190.37452711ss"
E1224 11:22:32.815185       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:22:32.815079745" prevR="190.38558837ss" incrR="184467440737.09550124ss" currentR="190.38557345ss"
E1224 11:22:32.821083       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:22:32.821014756" prevR="190.39320111ss" incrR="184467440737.09550261ss" currentR="190.39318756ss"
I1224 11:22:52.649110       1 trace.go:205] Trace[1372170232]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:c2fe1336-082e-4b3c-befd-2dfd9db95b4f,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 11:22:51.692) (total time: 956ms):
Trace[1372170232]: ---"About to write a response" 956ms (11:22:52.648)
Trace[1372170232]: [956.900453ms] [956.900453ms] END
E1224 11:23:02.978682       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:23:02.978647018" prevR="190.51986842ss" incrR="184467440737.09551563ss" currentR="190.51986789ss"
E1224 11:23:33.016640       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:23:33.016561634" prevR="190.64977524ss" incrR="184467440737.09548919ss" currentR="190.64974827ss"
E1224 11:23:33.022231       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:23:33.022181428" prevR="190.65723508ss" incrR="184467440737.09550266ss" currentR="190.65722158ss"
E1224 11:24:04.055153       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:24:04.055037592" prevR="191.17451715ss" incrR="184467440737.09550605ss" currentR="191.17450704ss"
E1224 11:24:34.184147       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:24:34.184045721" prevR="191.40547438ss" incrR="184467440737.09550400ss" currentR="191.40546222ss"
E1224 11:24:34.185957       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:24:34.185926798" prevR="191.40693502ss" incrR="184467440737.09551510ss" currentR="191.40693396ss"
E1224 11:24:34.187306       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:24:34.187251502" prevR="191.40778112ss" incrR="184467440737.09551459ss" currentR="191.40777955ss"
I1224 11:24:40.650245       1 trace.go:205] Trace[1872124967]: "GuaranteedUpdate etcd3" audit-id:907f9fc7-2adf-4464-b1ac-eeab41f65b0d,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints (24-Dec-2022 11:24:40.068) (total time: 580ms):
Trace[1872124967]: ---"Txn call finished" err:<nil> 555ms (11:24:40.649)
Trace[1872124967]: [580.269864ms] [580.269864ms] END
I1224 11:24:40.652722       1 trace.go:205] Trace[799491482]: "Update" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:907f9fc7-2adf-4464-b1ac-eeab41f65b0d,client:10.0.2.15,accept:application/json, */*,protocol:HTTP/2.0 (24-Dec-2022 11:24:40.045) (total time: 607ms):
Trace[799491482]: ---"Write to database call finished" len:1354,err:<nil> 582ms (11:24:40.651)
Trace[799491482]: [607.572575ms] [607.572575ms] END
E1224 11:25:04.761153       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:25:04.761051056" prevR="191.94290378ss" incrR="184467440737.09551331ss" currentR="191.94290093ss"
I1224 11:25:07.640303       1 trace.go:205] Trace[1295382880]: "Get" url:/api/v1/namespaces/kube-node-lease,user-agent:kube-apiserver/v1.25.3 (linux/amd64) kubernetes/434bfd8,audit-id:fcf30b26-0445-4a9b-9033-34b1c582bd54,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (24-Dec-2022 11:25:06.847) (total time: 792ms):
Trace[1295382880]: ---"About to write a response" 791ms (11:25:07.638)
Trace[1295382880]: [792.445356ms] [792.445356ms] END
E1224 11:26:05.353566       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:26:05.353467444" prevR="192.58957800ss" incrR="184467440737.09550241ss" currentR="192.58956425ss"
E1224 11:26:35.451873       1 queueset.go:440] "Overflow" err="queueset::currentR overflow" QS="workload-high" when="2022-12-24 11:26:35.451802296" prevR="192.91120043ss" incrR="184467440737.09550757ss" currentR="192.91119184ss"

* 
* ==> kube-controller-manager [1111d9a540af] <==
* I1224 07:06:54.350985       1 serving.go:348] Generated self-signed cert in-memory
I1224 07:06:56.006400       1 controllermanager.go:178] Version: v1.25.3
I1224 07:06:56.011069       1 controllermanager.go:180] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1224 07:06:56.060179       1 secure_serving.go:210] Serving securely on 127.0.0.1:10257
I1224 07:06:56.080746       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1224 07:06:56.080877       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1224 07:06:56.082423       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
F1224 07:07:19.562238       1 controllermanager.go:221] error building controller context: failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: Get "https://192.168.59.102:8443/healthz": net/http: TLS handshake timeout

* 
* ==> kube-controller-manager [33c691fe5213] <==
* I1224 07:08:26.863982       1 shared_informer.go:262] Caches are synced for attach detach
E1224 07:08:27.153955       1 memcache.go:206] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
E1224 07:08:27.358756       1 memcache.go:104] couldn't get resource list for metrics.k8s.io/v1beta1: the server is currently unable to handle the request
I1224 07:08:27.546190       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
W1224 07:08:27.802134       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/users-pg-db-service", retrying. Error: EndpointSlice informer cache is out of date
I1224 07:08:27.855909       1 shared_informer.go:262] Caches are synced for garbage collector
I1224 07:08:27.897484       1 shared_informer.go:262] Caches are synced for garbage collector
I1224 07:08:27.897527       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
W1224 07:08:27.934719       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "kube-system/metrics-server", retrying. Error: EndpointSlice informer cache is out of date
E1224 07:08:57.278439       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1224 07:09:00.633343       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1224 07:09:27.358529       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1224 07:09:30.864923       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1224 07:09:57.449519       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1224 07:10:01.039139       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1224 09:14:56.388167       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized
W1224 09:14:56.391741       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
E1224 09:21:35.681090       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1224 09:21:35.882099       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1224 09:22:06.000329       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1224 09:22:08.178735       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
I1224 09:43:54.640199       1 event.go:294] "Event occurred" object="default/users-app-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set users-app-deployment-58b778899c to 1"
I1224 09:43:54.828866       1 event.go:294] "Event occurred" object="default/users-app-deployment-58b778899c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: users-app-deployment-58b778899c-7nhhl"
I1224 09:44:17.555697       1 event.go:294] "Event occurred" object="default/users-app-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set users-app-deployment-577c6f88fb to 1 from 2"
I1224 09:44:17.800803       1 event.go:294] "Event occurred" object="default/users-app-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set users-app-deployment-58b778899c to 2 from 1"
I1224 09:44:18.012316       1 event.go:294] "Event occurred" object="default/users-app-deployment-58b778899c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: users-app-deployment-58b778899c-pv7w9"
I1224 09:44:18.013620       1 event.go:294] "Event occurred" object="default/users-app-deployment-577c6f88fb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: users-app-deployment-577c6f88fb-9q79r"
I1224 09:44:44.678878       1 event.go:294] "Event occurred" object="default/users-app-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set users-app-deployment-577c6f88fb to 0 from 1"
I1224 09:44:44.937778       1 event.go:294] "Event occurred" object="default/users-app-deployment-577c6f88fb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: users-app-deployment-577c6f88fb-cf45x"
I1224 10:19:14.118921       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:19:14.496555       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5959f988fd to 1"
I1224 10:19:15.422803       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:19:15.842069       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-gg2rb"
I1224 10:19:15.844913       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:19:15.847362       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5959f988fd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5959f988fd-vql45"
I1224 10:19:16.055044       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:19:16.057915       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:19:16.277001       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-nc8vg"
I1224 10:19:17.539237       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:19:17.757036       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:19:18.102389       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:19:24.804721       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:19:27.988049       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:20:26.787011       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:20:27.539688       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:20:40.953252       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:20:42.361041       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:20:44.654536       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:20:45.110570       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1224 10:20:45.117300       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:20:45.271606       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1224 10:21:12.451295       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:21:13.631750       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:21:13.724514       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1224 10:21:13.758593       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1224 10:21:13.919666       1 job_controller.go:510] enqueueing job ingress-nginx/ingress-nginx-admission-patch
E1224 10:24:27.363579       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1224 10:24:27.548247       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]
E1224 10:24:57.614128       1 resource_quota_controller.go:417] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
W1224 10:24:57.962509       1 garbagecollector.go:752] failed to discover some groups: map[metrics.k8s.io/v1beta1:the server is currently unable to handle the request]

* 
* ==> kube-proxy [9fcd8e098d87] <==
* I1224 07:09:20.623451       1 node.go:163] Successfully retrieved node IP: 192.168.59.102
I1224 07:09:20.623786       1 server_others.go:138] "Detected node IP" address="192.168.59.102"
I1224 07:09:20.634500       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1224 07:09:25.822740       1 server_others.go:199] "kube-proxy running in single-stack mode, this ipFamily is not supported" ipFamily=IPv6
I1224 07:09:25.840291       1 server_others.go:206] "Using iptables Proxier"
I1224 07:09:25.884718       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1224 07:09:25.891136       1 server.go:661] "Version info" version="v1.25.3"
I1224 07:09:25.917674       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1224 07:09:25.996242       1 config.go:317] "Starting service config controller"
I1224 07:09:25.996550       1 shared_informer.go:255] Waiting for caches to sync for service config
I1224 07:09:25.996635       1 config.go:226] "Starting endpoint slice config controller"
I1224 07:09:25.996646       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1224 07:09:26.079308       1 config.go:444] "Starting node config controller"
I1224 07:09:26.082418       1 shared_informer.go:255] Waiting for caches to sync for node config
I1224 07:09:26.789902       1 shared_informer.go:262] Caches are synced for node config
I1224 07:09:26.875154       1 shared_informer.go:262] Caches are synced for service config
I1224 07:09:26.973588       1 shared_informer.go:262] Caches are synced for endpoint slice config
I1224 11:12:23.385974       1 trace.go:205] Trace[22343982]: "iptables ChainExists" (24-Dec-2022 11:12:19.753) (total time: 2035ms):
Trace[22343982]: [2.035895356s] [2.035895356s] END

* 
* ==> kube-scheduler [0d83ac1df18b] <==
* Trace[1381387999]: ---"Objects listed" error:Get "https://192.168.59.102:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10008ms (06:05:01.797)
Trace[1381387999]: [10.012783111s] [10.012783111s] END
E1224 06:05:01.802701       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.59.102:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 06:05:01.976338       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: Get "https://192.168.59.102:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 06:05:01.976917       1 trace.go:205] Trace[1848884311]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 06:04:51.933) (total time: 10042ms):
Trace[1848884311]: ---"Objects listed" error:Get "https://192.168.59.102:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10041ms (06:05:01.975)
Trace[1848884311]: [10.042948153s] [10.042948153s] END
E1224 06:05:01.976944       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.59.102:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 06:05:01.987021       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.59.102:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 06:05:01.987994       1 trace.go:205] Trace[1068063258]: "Reflector ListAndWatch" name:pkg/server/dynamiccertificates/configmap_cafile_content.go:206 (24-Dec-2022 06:04:51.979) (total time: 10008ms):
Trace[1068063258]: ---"Objects listed" error:Get "https://192.168.59.102:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": net/http: TLS handshake timeout 10007ms (06:05:01.987)
Trace[1068063258]: [10.008136317s] [10.008136317s] END
E1224 06:05:01.988038       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.59.102:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 06:05:02.041062       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: Get "https://192.168.59.102:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 06:05:02.042318       1 trace.go:205] Trace[278296698]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 06:04:51.957) (total time: 10084ms):
Trace[278296698]: ---"Objects listed" error:Get "https://192.168.59.102:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10083ms (06:05:02.041)
Trace[278296698]: [10.08446456s] [10.08446456s] END
E1224 06:05:02.043027       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.59.102:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 06:05:02.229625       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.59.102:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 06:05:02.230815       1 trace.go:205] Trace[1990000016]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 06:04:52.219) (total time: 10010ms):
Trace[1990000016]: ---"Objects listed" error:Get "https://192.168.59.102:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10009ms (06:05:02.228)
Trace[1990000016]: [10.010942241s] [10.010942241s] END
E1224 06:05:02.231417       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.59.102:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 06:05:11.732663       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get "https://192.168.59.102:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 06:05:11.743866       1 trace.go:205] Trace[30010266]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 06:05:01.706) (total time: 10026ms):
Trace[30010266]: ---"Objects listed" error:Get "https://192.168.59.102:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10026ms (06:05:11.732)
Trace[30010266]: [10.026510363s] [10.026510363s] END
E1224 06:05:11.770003       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.59.102:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 06:05:12.612008       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1224 06:05:12.612885       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1224 06:05:12.638095       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1224 06:05:12.642986       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1224 06:05:12.643440       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1224 06:05:12.643473       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1224 06:05:12.643714       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1224 06:05:12.643744       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1224 06:05:12.643928       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1224 06:05:12.643956       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1224 06:05:12.644153       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1224 06:05:12.644179       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1224 06:05:12.644317       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
I1224 06:05:12.644384       1 trace.go:205] Trace[1679715446]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 06:05:02.299) (total time: 10344ms):
Trace[1679715446]: ---"Objects listed" error:csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope 10344ms (06:05:12.644)
Trace[1679715446]: [10.344551765s] [10.344551765s] END
E1224 06:05:12.644414       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1224 06:05:12.644582       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1224 06:05:12.644608       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1224 06:05:12.644756       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1224 06:05:12.644782       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1224 06:05:12.658079       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1224 06:05:12.658268       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1224 06:05:12.666864       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1224 06:05:12.682802       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1224 06:05:12.775996       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1224 06:05:12.780693       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1224 06:05:12.777306       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1224 06:05:12.786090       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1224 06:05:12.777850       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1224 06:05:12.786913       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
I1224 06:05:25.259349       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [27b14bea83e8] <==
* E1224 07:07:10.761711       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.59.102:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 07:07:10.837625       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.59.102:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 07:07:10.841441       1 trace.go:205] Trace[1455628810]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 07:07:00.832) (total time: 10008ms):
Trace[1455628810]: ---"Objects listed" error:Get "https://192.168.59.102:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10005ms (07:07:10.837)
Trace[1455628810]: [10.008942422s] [10.008942422s] END
E1224 07:07:10.843199       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.59.102:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 07:07:11.133039       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get "https://192.168.59.102:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 07:07:11.133170       1 trace.go:205] Trace[1807435285]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 07:07:01.060) (total time: 10073ms):
Trace[1807435285]: ---"Objects listed" error:Get "https://192.168.59.102:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10072ms (07:07:11.133)
Trace[1807435285]: [10.073051746s] [10.073051746s] END
E1224 07:07:11.133211       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.59.102:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 07:07:11.229359       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Get "https://192.168.59.102:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 07:07:11.235593       1 trace.go:205] Trace[284619264]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 07:07:01.167) (total time: 10068ms):
Trace[284619264]: ---"Objects listed" error:Get "https://192.168.59.102:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10061ms (07:07:11.228)
Trace[284619264]: [10.068355783s] [10.068355783s] END
E1224 07:07:11.240896       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.59.102:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 07:07:11.363464       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get "https://192.168.59.102:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 07:07:11.372466       1 trace.go:205] Trace[272487814]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 07:07:01.332) (total time: 10039ms):
Trace[272487814]: ---"Objects listed" error:Get "https://192.168.59.102:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10030ms (07:07:11.363)
Trace[272487814]: [10.039663719s] [10.039663719s] END
E1224 07:07:11.375380       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.59.102:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 07:07:11.443013       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.59.102:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1224 07:07:11.443488       1 trace.go:205] Trace[1570918886]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:134 (24-Dec-2022 07:07:01.436) (total time: 10006ms):
Trace[1570918886]: ---"Objects listed" error:Get "https://192.168.59.102:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout 10006ms (07:07:11.442)
Trace[1570918886]: [10.006777756s] [10.006777756s] END
E1224 07:07:11.443776       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.59.102:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1224 07:07:22.408472       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1224 07:07:22.408547       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1224 07:07:22.503927       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1224 07:07:22.503988       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1224 07:07:22.504100       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1224 07:07:22.504127       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1224 07:07:22.504209       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1224 07:07:22.504231       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1224 07:07:22.504307       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1224 07:07:22.504328       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1224 07:07:22.504397       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1224 07:07:22.504416       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1224 07:07:22.504480       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1224 07:07:22.504504       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1224 07:07:22.504565       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1224 07:07:22.504586       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1224 07:07:22.504657       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1224 07:07:22.504678       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1224 07:07:22.504788       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1224 07:07:22.504900       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1224 07:07:22.505300       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1224 07:07:22.505353       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1224 07:07:22.505492       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1224 07:07:22.505530       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1224 07:07:22.505782       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1224 07:07:22.505826       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1224 07:07:22.505855       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1224 07:07:22.505882       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1224 07:07:22.505953       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1224 07:07:22.505994       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
I1224 07:07:33.002714       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1224 10:19:16.129676       1 trace.go:205] Trace[1678613657]: "Scheduling" namespace:ingress-nginx,name:ingress-nginx-controller-5959f988fd-vql45 (24-Dec-2022 10:19:15.823) (total time: 134ms):
Trace[1678613657]: ---"Computing predicates done" 134ms (10:19:15.958)
Trace[1678613657]: [134.240116ms] [134.240116ms] END

* 
* ==> kubelet <==
* -- Journal begins at Sat 2022-12-24 07:04:37 UTC, ends at Sat 2022-12-24 11:27:31 UTC. --
Dec 24 11:09:45 minikube kubelet[1348]: E1224 11:09:45.722911    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:09:58 minikube kubelet[1348]: E1224 11:09:58.184848    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:10:11 minikube kubelet[1348]: E1224 11:10:11.149298    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:10:26 minikube kubelet[1348]: E1224 11:10:26.214681    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:10:39 minikube kubelet[1348]: E1224 11:10:39.081164    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:10:52 minikube kubelet[1348]: E1224 11:10:52.678581    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:11:03 minikube kubelet[1348]: E1224 11:11:03.815569    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:11:17 minikube kubelet[1348]: E1224 11:11:17.911662    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:11:32 minikube kubelet[1348]: E1224 11:11:32.545224    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:13:54 minikube kubelet[1348]: E1224 11:13:54.707751    1348 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Dec 24 11:13:54 minikube kubelet[1348]: E1224 11:13:54.758799    1348 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Dec 24 11:13:54 minikube kubelet[1348]: E1224 11:13:54.760826    1348 kuberuntime_manager.go:862] container &Container{Name:controller,Image:k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8,Command:[],Args:[/nginx-ingress-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --watch-ingress-without-class=true --configmap=$(POD_NAMESPACE)/ingress-nginx-controller --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services --udp-services-configmap=$(POD_NAMESPACE)/udp-services --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:80,ContainerPort:80,Protocol:TCP,HostIP:,},ContainerPort{Name:https,HostPort:443,ContainerPort:443,Protocol:TCP,HostIP:,},ContainerPort{Name:webhook,HostPort:0,ContainerPort:8443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:LD_PRELOAD,Value:/usr/local/lib/libmimalloc.so,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{94371840 0} {<nil>} 90Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:webhook-cert,ReadOnly:true,MountPath:/usr/local/certificates/,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-9xv46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:&Lifecycle{PostStart:nil,PreStop:&LifecycleHandler{Exec:&ExecAction{Command:[/wait-shutdown],},HTTPGet:nil,TCPSocket:nil,},},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[ALL],},Privileged:nil,SELinuxOptions:nil,RunAsUser:*101,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*true,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod ingress-nginx-controller-5959f988fd-vql45_ingress-nginx(7c7efba0-23d4-48e7-957d-da057236a22e): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Dec 24 11:13:54 minikube kubelet[1348]: E1224 11:13:54.812556    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:14:07 minikube kubelet[1348]: E1224 11:14:07.246004    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:14:21 minikube kubelet[1348]: E1224 11:14:21.467014    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:14:36 minikube kubelet[1348]: E1224 11:14:36.054627    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:14:50 minikube kubelet[1348]: E1224 11:14:50.255385    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:15:01 minikube kubelet[1348]: E1224 11:15:01.838426    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:15:14 minikube kubelet[1348]: E1224 11:15:14.811837    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:15:29 minikube kubelet[1348]: E1224 11:15:29.453580    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:15:40 minikube kubelet[1348]: E1224 11:15:40.701962    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:15:52 minikube kubelet[1348]: E1224 11:15:52.855790    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:16:05 minikube kubelet[1348]: E1224 11:16:05.705243    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:16:20 minikube kubelet[1348]: E1224 11:16:20.955468    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:16:32 minikube kubelet[1348]: E1224 11:16:32.744299    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:16:47 minikube kubelet[1348]: E1224 11:16:47.940553    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:16:58 minikube kubelet[1348]: E1224 11:16:58.662539    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:17:15 minikube kubelet[1348]: E1224 11:17:15.667514    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:17:31 minikube kubelet[1348]: E1224 11:17:31.302049    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:17:44 minikube kubelet[1348]: E1224 11:17:44.772155    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:17:55 minikube kubelet[1348]: E1224 11:17:55.642245    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:18:08 minikube kubelet[1348]: E1224 11:18:08.778811    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:18:20 minikube kubelet[1348]: E1224 11:18:20.461772    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:18:33 minikube kubelet[1348]: E1224 11:18:33.672819    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:18:48 minikube kubelet[1348]: E1224 11:18:48.704221    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:21:00 minikube kubelet[1348]: E1224 11:21:00.849253    1348 remote_image.go:222] "PullImage from image service failed" err="rpc error: code = Unknown desc = context deadline exceeded" image="k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Dec 24 11:21:00 minikube kubelet[1348]: E1224 11:21:00.849393    1348 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = context deadline exceeded" image="k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8"
Dec 24 11:21:00 minikube kubelet[1348]: E1224 11:21:00.849713    1348 kuberuntime_manager.go:862] container &Container{Name:controller,Image:k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8,Command:[],Args:[/nginx-ingress-controller --election-id=ingress-controller-leader --controller-class=k8s.io/ingress-nginx --watch-ingress-without-class=true --configmap=$(POD_NAMESPACE)/ingress-nginx-controller --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services --udp-services-configmap=$(POD_NAMESPACE)/udp-services --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:80,ContainerPort:80,Protocol:TCP,HostIP:,},ContainerPort{Name:https,HostPort:443,ContainerPort:443,Protocol:TCP,HostIP:,},ContainerPort{Name:webhook,HostPort:0,ContainerPort:8443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:LD_PRELOAD,Value:/usr/local/lib/libmimalloc.so,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{94371840 0} {<nil>} 90Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:webhook-cert,ReadOnly:true,MountPath:/usr/local/certificates/,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-9xv46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:&Lifecycle{PostStart:nil,PreStop:&LifecycleHandler{Exec:&ExecAction{Command:[/wait-shutdown],},HTTPGet:nil,TCPSocket:nil,},},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[ALL],},Privileged:nil,SELinuxOptions:nil,RunAsUser:*101,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*true,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod ingress-nginx-controller-5959f988fd-vql45_ingress-nginx(7c7efba0-23d4-48e7-957d-da057236a22e): ErrImagePull: rpc error: code = Unknown desc = context deadline exceeded
Dec 24 11:21:00 minikube kubelet[1348]: E1224 11:21:00.849797    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ErrImagePull: \"rpc error: code = Unknown desc = context deadline exceeded\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:21:15 minikube kubelet[1348]: E1224 11:21:15.003791    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:21:26 minikube kubelet[1348]: E1224 11:21:26.747897    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:21:40 minikube kubelet[1348]: E1224 11:21:40.630047    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:21:51 minikube kubelet[1348]: E1224 11:21:51.744997    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:22:06 minikube kubelet[1348]: E1224 11:22:06.668582    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:22:22 minikube kubelet[1348]: E1224 11:22:22.530579    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:22:35 minikube kubelet[1348]: E1224 11:22:35.698662    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:22:49 minikube kubelet[1348]: E1224 11:22:49.896264    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:23:05 minikube kubelet[1348]: E1224 11:23:05.752208    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:23:17 minikube kubelet[1348]: E1224 11:23:17.641906    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:23:30 minikube kubelet[1348]: E1224 11:23:30.641265    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:23:43 minikube kubelet[1348]: E1224 11:23:43.796473    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:23:57 minikube kubelet[1348]: E1224 11:23:57.712861    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:24:12 minikube kubelet[1348]: E1224 11:24:12.656826    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:24:25 minikube kubelet[1348]: E1224 11:24:25.634025    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:24:38 minikube kubelet[1348]: E1224 11:24:38.787340    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:24:50 minikube kubelet[1348]: E1224 11:24:50.219550    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:25:04 minikube kubelet[1348]: E1224 11:25:04.706038    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:25:19 minikube kubelet[1348]: E1224 11:25:19.963390    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:25:37 minikube kubelet[1348]: E1224 11:25:37.006772    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e
Dec 24 11:25:48 minikube kubelet[1348]: E1224 11:25:48.689590    1348 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ImagePullBackOff: \"Back-off pulling image \\\"k8s.gcr.io/ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8\\\"\"" pod="ingress-nginx/ingress-nginx-controller-5959f988fd-vql45" podUID=7c7efba0-23d4-48e7-957d-da057236a22e

* 
* ==> kubernetes-dashboard [577a681cbab9] <==
* 2022/12/24 06:21:13 Getting list of namespaces
2022/12/24 06:21:13 [2022-12-24T06:21:13Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:16 [2022-12-24T06:21:16Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/24 06:21:16 Getting list of all services in the cluster
2022/12/24 06:21:16 [2022-12-24T06:21:16Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:18 [2022-12-24T06:21:18Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:18 Getting list of namespaces
2022/12/24 06:21:18 [2022-12-24T06:21:18Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:22 [2022-12-24T06:21:22Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/24 06:21:22 Getting list of all services in the cluster
2022/12/24 06:21:22 [2022-12-24T06:21:22Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:23 [2022-12-24T06:21:23Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:23 Getting list of namespaces
2022/12/24 06:21:23 [2022-12-24T06:21:23Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:26 [2022-12-24T06:21:26Z] Incoming HTTP/1.1 GET /api/v1/service/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/24 06:21:26 Getting list of all services in the cluster
2022/12/24 06:21:27 [2022-12-24T06:21:27Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:28 [2022-12-24T06:21:28Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:28 Getting list of namespaces
2022/12/24 06:21:29 [2022-12-24T06:21:29Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:31 [2022-12-24T06:21:31Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2022/12/24 06:21:31 [2022-12-24T06:21:31Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:33 [2022-12-24T06:21:33Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:33 Getting list of namespaces
2022/12/24 06:21:33 [2022-12-24T06:21:33Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:38 [2022-12-24T06:21:38Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2022/12/24 06:21:38 [2022-12-24T06:21:38Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:38 [2022-12-24T06:21:38Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:38 Getting list of namespaces
2022/12/24 06:21:38 [2022-12-24T06:21:38Z] Incoming HTTP/1.1 GET /api/v1/settings/global request from 127.0.0.1: 
2022/12/24 06:21:38 [2022-12-24T06:21:38Z] Incoming HTTP/1.1 GET /api/v1/settings/global/cani request from 127.0.0.1: 
2022/12/24 06:21:39 [2022-12-24T06:21:39Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:39 [2022-12-24T06:21:39Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:39 [2022-12-24T06:21:39Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:39 Getting list of namespaces
2022/12/24 06:21:39 [2022-12-24T06:21:39Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:39 [2022-12-24T06:21:39Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:40 [2022-12-24T06:21:40Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:40 Getting list of namespaces
2022/12/24 06:21:40 [2022-12-24T06:21:40Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:40 [2022-12-24T06:21:40Z] Incoming HTTP/1.1 GET /api/v1/settings/global request from 127.0.0.1: 
2022/12/24 06:21:40 [2022-12-24T06:21:40Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:44 [2022-12-24T06:21:44Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:44 Getting list of namespaces
2022/12/24 06:21:44 [2022-12-24T06:21:44Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:44 [2022-12-24T06:21:44Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:44 Getting list of namespaces
2022/12/24 06:21:44 [2022-12-24T06:21:44Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:49 [2022-12-24T06:21:49Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:49 Getting list of namespaces
2022/12/24 06:21:49 [2022-12-24T06:21:49Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:49 [2022-12-24T06:21:49Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:49 Getting list of namespaces
2022/12/24 06:21:49 [2022-12-24T06:21:49Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:54 [2022-12-24T06:21:54Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:54 Getting list of namespaces
2022/12/24 06:21:54 [2022-12-24T06:21:54Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 06:21:54 [2022-12-24T06:21:54Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 06:21:54 Getting list of namespaces
2022/12/24 06:21:54 [2022-12-24T06:21:54Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> kubernetes-dashboard [9c37402a2a5f] <==
* 2022/12/24 09:51:44 [2022-12-24T09:51:44Z] Incoming HTTP/1.1 GET /api/v1/configmap/default/users-config request from 127.0.0.1: 
2022/12/24 09:51:44 Getting details of users-config config map in default namespace
2022/12/24 09:51:44 [2022-12-24T09:51:44Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:46 [2022-12-24T09:51:46Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 09:51:46 Getting list of namespaces
2022/12/24 09:51:46 [2022-12-24T09:51:46Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:49 [2022-12-24T09:51:49Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2022/12/24 09:51:49 [2022-12-24T09:51:49Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:49 [2022-12-24T09:51:49Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2022/12/24 09:51:49 Getting events related to a pod in namespace
2022/12/24 09:51:49 [2022-12-24T09:51:49Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9 request from 127.0.0.1: 
2022/12/24 09:51:50 Getting details of users-app-deployment-58b778899c-pv7w9 pod in default namespace
2022/12/24 09:51:50 [2022-12-24T09:51:50Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:51 No persistentvolumeclaims found related to users-app-deployment-58b778899c-pv7w9 pod
2022/12/24 09:51:51 [2022-12-24T09:51:51Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 09:51:51 Getting list of namespaces
2022/12/24 09:51:51 [2022-12-24T09:51:51Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:53 [2022-12-24T09:51:53Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:53 [2022-12-24T09:51:53Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/24 09:51:53 No persistentvolumeclaims found related to users-app-deployment-58b778899c-pv7w9 pod
2022/12/24 09:51:53 [2022-12-24T09:51:53Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:54 [2022-12-24T09:51:54Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/24 09:51:54 [2022-12-24T09:51:54Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2022/12/24 09:51:54 [2022-12-24T09:51:54Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9 request from 127.0.0.1: 
2022/12/24 09:51:54 [2022-12-24T09:51:54Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 09:51:54 Getting list of namespaces
2022/12/24 09:51:54 Getting events related to a pod in namespace
2022/12/24 09:51:54 Getting details of users-app-deployment-58b778899c-pv7w9 pod in default namespace
2022/12/24 09:51:54 No persistentvolumeclaims found related to users-app-deployment-58b778899c-pv7w9 pod
2022/12/24 09:51:54 [2022-12-24T09:51:54Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:54 [2022-12-24T09:51:54Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:54 [2022-12-24T09:51:54Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:51:55 No persistentvolumeclaims found related to users-app-deployment-58b778899c-pv7w9 pod
2022/12/24 09:51:55 [2022-12-24T09:51:55Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:52:15 [2022-12-24T09:52:15Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/24 09:52:15 [2022-12-24T09:52:15Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2022/12/24 09:52:15 Getting events related to a pod in namespace
2022/12/24 09:52:15 [2022-12-24T09:52:15Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9 request from 127.0.0.1: 
2022/12/24 09:52:15 [2022-12-24T09:52:15Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 09:52:15 [2022-12-24T09:52:15Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:52:15 No persistentvolumeclaims found related to users-app-deployment-58b778899c-pv7w9 pod
2022/12/24 09:52:15 Getting list of namespaces
2022/12/24 09:52:15 Getting details of users-app-deployment-58b778899c-pv7w9 pod in default namespace
2022/12/24 09:52:15 [2022-12-24T09:52:15Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:52:15 [2022-12-24T09:52:15Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:52:17 No persistentvolumeclaims found related to users-app-deployment-58b778899c-pv7w9 pod
2022/12/24 09:52:18 [2022-12-24T09:52:18Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:52:18 [2022-12-24T09:52:18Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9/persistentvolumeclaim?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2022/12/24 09:52:18 [2022-12-24T09:52:18Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2022/12/24 09:52:18 Getting events related to a pod in namespace
2022/12/24 09:52:18 [2022-12-24T09:52:18Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2022/12/24 09:52:18 Getting list of namespaces
2022/12/24 09:52:18 No persistentvolumeclaims found related to users-app-deployment-58b778899c-pv7w9 pod
2022/12/24 09:52:18 [2022-12-24T09:52:18Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:52:18 [2022-12-24T09:52:18Z] Incoming HTTP/1.1 GET /api/v1/pod/default/users-app-deployment-58b778899c-pv7w9 request from 127.0.0.1: 
2022/12/24 09:52:18 Getting details of users-app-deployment-58b778899c-pv7w9 pod in default namespace
2022/12/24 09:52:19 [2022-12-24T09:52:18Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:52:19 [2022-12-24T09:52:19Z] Outcoming response to 127.0.0.1 with 200 status code
2022/12/24 09:52:20 No persistentvolumeclaims found related to users-app-deployment-58b778899c-pv7w9 pod
2022/12/24 09:52:20 [2022-12-24T09:52:20Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [a6fe6cb83c72] <==
* I1224 07:09:53.364737       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1224 07:09:55.985791       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1224 07:09:56.001368       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1224 07:10:14.857109       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1224 07:10:14.859823       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"aafd13b4-f02b-4e4c-a3cf-9131ebd2aafc", APIVersion:"v1", ResourceVersion:"30550", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_bb871d28-9e42-439b-a9e7-8e3ea68867f8 became leader
I1224 07:10:14.860831       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_bb871d28-9e42-439b-a9e7-8e3ea68867f8!
I1224 07:10:15.467305       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_bb871d28-9e42-439b-a9e7-8e3ea68867f8!
I1224 10:55:22.161998       1 request.go:655] Throttling request took 1.034684955s, request: GET:https://10.96.0.1:443/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath

